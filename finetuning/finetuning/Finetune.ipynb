{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 1803.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "import functools\n",
    "from finetune import _get_bfcl_tokenized_test_ds\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#         model_name = \"unsloth/Llama-3.2-1B-Instruct\", # or choose \"unsloth/Llama-3.2-1B\"\n",
    "#         # max_seq_length = max_seq_length,\n",
    "#         dtype = torch.bfloat16,\n",
    "#         load_in_4bit = False,\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "with open('test.json', 'r') as file:\n",
    "    test_data = json.load(file)\n",
    "test_ds = Dataset.from_list(test_data)\n",
    "test_ds = test_ds.map(functools.partial(_get_bfcl_tokenized_test_ds,tokenizer=tokenizer,json_or_yaml=\"json\"),batched=True,remove_columns=[\"function\",\"question\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 742.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_ds = test_ds.map(functools.partial(_get_bfcl_tokenized_test_ds,tokenizer=tokenizer,json_or_yaml=\"json\"),batched=True,remove_columns=[\"function\",\"question\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/recoverx/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/recoverx/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2024.9.post4: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.325 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.9.post4 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12125/12125 [00:01<00:00, 7162.49 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 344.41 examples/s]\n",
      "Map (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12125/12125 [00:03<00:00, 3237.74 examples/s]\n",
      "num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n",
      "Map (num_proc=5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.24 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12125/12125 [00:03<00:00, 3845.29 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 522.49 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 12,125 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 32 | Total steps = 1\n",
      " \"-____-\"     Number of trainable parameters = 22,544,384\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/1 : < :, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 419, 128256)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'rfind'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfinetune\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_finetune\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun_finetune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munsloth/Llama-3.2-1B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbfcl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson_or_yaml\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/astarag/Finetuning/finetuning/finetuning/finetune.py:100\u001b[0m, in \u001b[0;36mrun_finetune\u001b[0;34m(model_name, dataset_name, json_or_yaml)\u001b[0m\n\u001b[1;32m     61\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m     62\u001b[0m model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m     63\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m ),\n\u001b[1;32m     94\u001b[0m )\n\u001b[1;32m     95\u001b[0m trainer \u001b[38;5;241m=\u001b[39m train_on_responses_only(\n\u001b[1;32m     96\u001b[0m     trainer,\n\u001b[1;32m     97\u001b[0m     instruction_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|start_header_id|>system<|end_header_id|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     98\u001b[0m     response_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|start_header_id|>assistant<|end_header_id|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     99\u001b[0m )\n\u001b[0;32m--> 100\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer_stats, trainer, model, tokenizer\n",
      "File \u001b[0;32m<string>:142\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m<string>:440\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/transformers/trainer.py:2804\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2802\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2804\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   2807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/transformers/trainer.py:2761\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2760\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 2761\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2764\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/transformers/trainer.py:3666\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3663\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3665\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3666\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3667\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3669\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3670\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3674\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3676\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/transformers/trainer.py:3956\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3952\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3953\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[1;32m   3954\u001b[0m         )\n\u001b[1;32m   3955\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3956\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3958\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/astarag/Finetuning/finetuning/finetuning/finetune.py:111\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_preds)\u001b[0m\n\u001b[1;32m    107\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pred, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# Extract the model's answer from between the headers\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     pred_output \u001b[38;5;241m=\u001b[39m pred[\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrfind\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|end_header_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m19\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# Extract ground truth (assuming similar format)\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     gt_output \u001b[38;5;241m=\u001b[39m label[label\u001b[38;5;241m.\u001b[39mrfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|end_header_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m19\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'rfind'"
     ]
    }
   ],
   "source": [
    "from finetune import run_finetune\n",
    "run_finetune(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    dataset_name = \"bfcl\",\n",
    "    json_or_yaml = \"json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/recoverx/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/recoverx/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2024.9.post4: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.325 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.9.post4 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from unsloth.chat_templates import get_chat_template, train_on_responses_only\n",
    "\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "dtype = torch.bfloat16 \n",
    "load_in_4bit = False \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name, # or choose \"unsloth/Llama-3.2-1B\"\n",
    "    # max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                        \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "        lora_alpha = 64,\n",
    "        lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "        bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "        # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "        use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "        random_state = 3407,\n",
    "        use_rslora = False,  # We support rank stabilized LoRA\n",
    "        loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template, train_on_responses_only\n",
    "from datasets import Dataset\n",
    "import functools\n",
    "import json\n",
    "from finetune import _get_bfcl_tokenized_test_ds, _get_bfcl_train_tokenized_ds\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template = \"llama-3.1\",\n",
    "    )\n",
    "train_data = []\n",
    "with open('train.json', 'r') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line.strip())\n",
    "        json_obj['Functions'] = json_obj['Functions'][0] if isinstance(json_obj['Functions'],list) else json_obj['Functions']\n",
    "        json_obj['Output'] = json_obj['Output'][0] if isinstance(json_obj['Output'],list) else json_obj['Output']\n",
    "        train_data.append(json_obj)\n",
    "with open('test.json', 'r') as file:\n",
    "    test_data = json.load(file)\n",
    "# train_data = _process_bfcl_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Instruction': 'Can you provide me with images and videos of a specific location with latitude 40.712776 and longitude -74.005974?\\n',\n",
       " 'Functions': \"{'name': 'RapidAPI', 'api_name': 'requests.get', 'description': 'Geocoding places Info with images & videos.', 'parameters': [{'name': 'lat', 'description': 'Latitude in decimal degrees (wgs84)', 'type': 'NUMBER'}, {'name': 'lng', 'description': 'Longitude in decimal degrees (wgs84)', 'type': 'NUMBER'}, {'name': 'version', 'description': '', 'type': 'string'}, {'name': 'lang', 'description': 'Prefered language of content.', 'type': 'STRING'}]}\\n\",\n",
       " 'Output': 'response = requests.get(\"https://geocoding-places.p.rapidapi.com/get_geocoding_images/v1\", headers={\"X-RapidAPI-Key\": \"SIGN-UP-FOR-KEY\", \"X-RapidAPI-Host\": \"geocoding-places.p.rapidapi.com\"}, params={\"lat\": \"40.712776\", \"lng\": \"-74.005974\", \"version\": \"v1\", \"lang\": \"en\"})'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12125/12125 [00:01<00:00, 7058.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = Dataset.from_list(train_data)\n",
    "json_or_yaml = \"json\"\n",
    "train_ds = train_ds.map(functools.partial(_get_bfcl_train_tokenized_ds,tokenizer=tokenizer,json_or_yaml=json_or_yaml),batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMAINING WORK\n",
    "1. Evaluation for BFCL and xLAM dataset\n",
    "2. finetuning scripts and integration with weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12125/12125 [00:03<00:00, 3240.17 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12125/12125 [00:02<00:00, 4068.64 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 12,125 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 32 | Total steps = 5\n",
      " \"-____-\"     Number of trainable parameters = 22,544,384\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:06, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.783000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.833800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.602900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.350800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_ds,\n",
    "    dataset_text_field = \"prompt\",\n",
    "    max_seq_length = 3072,\n",
    "    dataset_num_proc = 8,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 32,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        # warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 5,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        save_safetensors=True\n",
    "    ),\n",
    ")\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>system<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['model_answer', 'prompt'],\n",
       "    num_rows: 112\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch_size = 16\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "for start in range(0,len(test_ds),val_batch_size):\n",
    "    end = min(len(test_ds),start+val_batch_size)\n",
    "    batch = test_ds[start:end]\n",
    "    prompts = batch['prompt']\n",
    "    inputs = tokenizer(prompts, return_tensors = \"pt\",padding=True,truncation=True,).to(\"cuda:0\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(input_ids = inputs['input_ids'], attention_mask = inputs['attention_mask'], max_new_tokens = 64, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "out = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coffee_shop.find_nearby(amenities={\"Wi-Fi\": true}, location={\"city\": \"San Francisco\", \"state\": \"CA\"})'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][out[0].rfind(\"<|end_header_id|>\",1)+19:].split(\"<|eot_id|>\")[0].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                                                                                                                                                                                                                            \\n\\ngcloud.alpha.anthos.export(\"my-cluster\", \"--project=my-project\", \"--output-directory=my-dir\")<|eot_id|>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[500][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:14<00:00,  2.01s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ModelReturn(model_answer='coffee_shop.find_nearby({\"amenities\": [\"Wi-Fi\"], \"location\": \"San Francisco\"})', gt_answer='coffee_shop.find_nearby(location=\"San Francisco\", amenities=\"Wi-Fi\")', score=0.0),\n",
       " ModelReturn(model_answer='flight.book(\"Los Angeles\", \"New York\", \"June 15th\")', gt_answer='flight.book(origin=\"Los Angeles\", destination=\"New York\", passengers=2, date=\"June 15th\")', score=0.0),\n",
       " ModelReturn(model_answer='restaurant.book_table(date=\"2024-07-26\", time=\"19:00\")', gt_answer='restaurant.book_table(restaurant=\"Italiano\\'s\", location=\"Manhattan, New York\", party_size=2, reservation_time=\"2023-10-23 19:00\")', score=0.0),\n",
       " ModelReturn(model_answer='weather.forecast(Paris, 5)', gt_answer='weather.forecast(location=\"Paris, France\", days=5)', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='pharmacy.find_nearby(location=\"San Diego, California\", feature=\"Drive-thru\")', score=0.0),\n",
       " ModelReturn(model_answer='target.order(items=[{\"items\": {\"description\": \"Ordered items in a list.\", \"type\": \"array\"}, \"type\": \"string\"}, {\"items\": {\"description\": \"Ordered items in a list.\", \"type\": \"array\"}, \"type\": \"string\"}], quantity={\"items\": {\"description\": \"Quantity of the order corresponding to the previous item variable.\", \"type\": \"integer\"}, \"type\": \"array\"}, currency=\"USD\")', gt_answer='target.order(loc=Berkeley,item=[\"potato\", \"chocolate\"], quantity=[6,8])', score=0.0),\n",
       " ModelReturn(model_answer='classes.find(\"Berkeley\", \"Computer Science\", \"undergraduate\")', gt_answer=\"classes.find(school='Berkeley', discipline='computer science', level='undergraduate', status='open')\", score=0.0),\n",
       " ModelReturn(model_answer='gas_station.find_nearby(location={\"description\": \"Current location for the search.\", \"type\": \"string\"})', gt_answer='gas_station.find_nearby(location=\"current\")', score=0.0),\n",
       " ModelReturn(model_answer='restaurant.find_top_rated(amenities={\"Outdoor Seating\": True, \"Vegetarian Options\": True}, location=\"San Francisco\")', gt_answer='restaurant.find_top_rated(location=\"San Francisco\", amenities=[\"Outdoor Seating\"])', score=0.5),\n",
       " ModelReturn(model_answer='timezone.get_current_time()', gt_answer='timezone.get_current_time(timezone=\"America/New_York\")', score=0.0),\n",
       " ModelReturn(model_answer='lyrics.find(\"Shape of You\", \"Ed Sheeran\")', gt_answer='lyrics.find(song=\"Shape of You\", artist=\"Ed Sheeran\")', score=0.0),\n",
       " ModelReturn(model_answer='news.get_headlines(\"CNN\")', gt_answer='news.get_headlines(source=\"CNN\")', score=0.0),\n",
       " ModelReturn(model_answer='dictionary.get_definition(\"serendipity\")', gt_answer='dictionary.get_definition(word=\"serendipity\")', score=0.0),\n",
       " ModelReturn(model_answer='unit_conversion.convert(10, \"miles\", \"kilometers\")', gt_answer='unit_conversion.convert(value=10, from_unit=\"miles\", to_unit=\"kilometers\")', score=0.0),\n",
       " ModelReturn(model_answer='currency_conversion.get_exchange_rate(base_currency=\"USD\", target_currency=\"EUR\")', gt_answer='currency_conversion.get_exchange_rate(base_currency=\"USD\", target_currency=\"EUR\")', score=1.0),\n",
       " ModelReturn(model_answer='movie.search(\"The Shawshank Redemption\")', gt_answer='movie.search(title=\"The Shawshank Redemption\")', score=0.0),\n",
       " ModelReturn(model_answer='stock.get_price(company=\"Apple Inc.\")', gt_answer='stock.get_price(company=\"Apple Inc.\")', score=1.0),\n",
       " ModelReturn(model_answer='park.find_nearby({\"amenities\": \"playground\", \"location\": \"Chicago, IL\"})', gt_answer='park.find_nearby(location=\"Chicago, Illinois\", features=[\"Playground\", \"Picnic Area\"])', score=0.0),\n",
       " ModelReturn(model_answer='translation.translate(text=\"Hello, world!\", target_language=\"Spanish\")', gt_answer='translation.translate(text=\"Hello, world!\", target_language=\"Spanish\")', score=1.0),\n",
       " ModelReturn(model_answer='', gt_answer='trail.find_nearby(location=\"Los Angeles, California\", features=[\"Dog-Friendly\"])', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='store.find_nearby(location=\"San Jose, California\", open_on_sundays=true)', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='restaurant.find_nearby(location=\"Austin, Texas\", cuisine=\"Vegan\", open_for_dinner=true)', score=0.0),\n",
       " ModelReturn(model_answer='timer.set(20, \"Check the oven\")', gt_answer='timer.set(duration=20, message=\"Check the oven\")', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='parking.find_nearby(location=\"Golden Gate Bridge, San Francisco\")', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='book.recommend(interest=\"Science Fiction\")', score=0.0),\n",
       " ModelReturn(model_answer='exchange_rate.get(\"USD\", \"EUR\")', gt_answer='exchange_rate.get(base_currency=\"USD\", target_currency=\"EUR\")', score=0.0),\n",
       " ModelReturn(model_answer='car_rental.search(\"Los Angeles\", \"2024-01-01\", \"2024-12-31\")', gt_answer='car_rental.search(location=\"Los Angeles\", start_date=\"2023-09-01\", end_date=\"2023-09-10\")', score=0.0),\n",
       " ModelReturn(model_answer='timezone.convert(\"Tokyo\", \"Asia/Tokyo\")', gt_answer='timezone.convert(location=\"Tokyo, Japan\", timezone=\"Asia/Tokyo\")', score=0.0),\n",
       " ModelReturn(model_answer='news.search(\"technology\")', gt_answer='news.search(topic=\"technology\")', score=0.0),\n",
       " ModelReturn(model_answer='atm.find_nearby(location={\"latitude\": 37.7749, \"longitude\": -122.4194}, amenities=[\"Cash Deposits\", \"24/7 Access\", \"Wheelchair Access\"], angle=45)', gt_answer='atm.find_nearby(location=\"current_location\", feature=\"Cash Deposits\")', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='hiking_trail.search(location=\"San Francisco\")', score=0.0),\n",
       " ModelReturn(model_answer='exchange_rate.get(\"USD\", \"EUR\")', gt_answer='exchange_rate.get(base_currency=\"USD\", target_currency=\"EUR\")', score=0.0),\n",
       " ModelReturn(model_answer='hotel.search(\"Miami, Florida\", {\"amenities\": [\"Pool\", \"Gym\"], \"type\": \"array\"})', gt_answer='hotel.search(location=\"Miami, Florida\", amenities=[\"Pool\", \"Gym\"])', score=0.0),\n",
       " ModelReturn(model_answer='timezone.convert(\"Tokyo, Japan\", \"America/New_York\")', gt_answer='timezone.convert(location=\"Tokyo, Japan\", target_timezone=\"Asia/Tokyo\")', score=0.0),\n",
       " ModelReturn(model_answer='news.get_headlines(\"CNN\")', gt_answer='news.get_headlines(news_source=\"CNN\")', score=0.0),\n",
       " ModelReturn(model_answer='recipe.search(\"spaghetti carbonara\")', gt_answer='recipe.search(keywords=\"spaghetti carbonara\", dietary_restrictions=[\"Vegetarian\"])', score=0.0),\n",
       " ModelReturn(model_answer='movie_theater.search(\"Los Angeles\", \"blockbuster\")', gt_answer='movie_theater.search(location=\"Los Angeles\", movie_title=\"Avengers: Endgame\")', score=0.0),\n",
       " ModelReturn(model_answer='finance.calculate_compound_interest(principal=5000, interest_rate=0.05, time_period=3)', gt_answer='finance.calculate_compound_interest(principal=5000, interest_rate=0.05, time_period=3)', score=1.0),\n",
       " ModelReturn(model_answer='finance.calculate_mortgage_payment(loan_amount=200000, interest_rate=0.04, loan_term=30)', gt_answer='finance.calculate_mortgage_payment(loan_amount=200000, interest_rate=0.04, loan_term=30)', score=1.0),\n",
       " ModelReturn(model_answer='finance.calculate_roi(investment_amount={\"description\": \"$10,000\", \"type\": \"number\"}, profit={\"description\": \"$2,500\", \"type\": \"number\"})', gt_answer='finance.calculate_roi(investment_amount=10000, profit=2500)', score=0.0),\n",
       " ModelReturn(model_answer='finance.calculate_present_value(future_cash_flow={\"amount\": 5000, \"rate\": 0.08, \"period\": 5}, discount_rate=0.08, time_period=5)', gt_answer='finance.calculate_present_value(future_cash_flow=5000, discount_rate=0.08, time_period=5)', score=0.6666666666666666),\n",
       " ModelReturn(model_answer='finance.calculate_car_loan_payment(loan_amount=20000, interest_rate=0.06, loan_term=5)', gt_answer='finance.calculate_car_loan_payment(loan_amount=20000, interest_rate=0.06, loan_term=5)', score=1.0),\n",
       " ModelReturn(model_answer='finance.calculate_future_value(5000, 0.03, 10)', gt_answer='finance.calculate_future_value(investment_amount=5000, interest_rate=0.03, time_period=10)', score=0.0),\n",
       " ModelReturn(model_answer='finance.calculate_minimum_payment(balance=5000, interest_rate=0.18)', gt_answer='finance.calculate_minimum_payment(balance=5000, interest_rate=0.18)', score=1.0),\n",
       " ModelReturn(model_answer='finance.calculate_roe(net_income=1000000, shareholders_equity=10000000)', gt_answer='finance.calculate_roe(net_income=1000000, shareholders_equity=10000000)', score=1.0),\n",
       " ModelReturn(model_answer='finance.calculate_personal_loan_payment(loan_amount=10000, interest_rate=0.08, loan_term=2)', gt_answer='finance.calculate_personal_loan_payment(loan_amount=10000, interest_rate=0.08, loan_term=2)', score=1.0),\n",
       " ModelReturn(model_answer='stock.average_price(date=\"2024-01-01\")', gt_answer='stock.average_price(symbol=\"AAPL\", start_date=\"2022-01-01\", end_date=\"2022-01-31\")', score=0.0),\n",
       " ModelReturn(model_answer='stock.simulate_walk(days=10)', gt_answer='stock.simulate_walk(symbol=\"AAPL\", days=10)', score=1.0),\n",
       " ModelReturn(model_answer='stock.correlation(\"Apple\", \"Microsoft\", \"2022-01-01\", \"2022-12-31\")', gt_answer='stock.correlation(symbol1=\"AAPL\", symbol2=\"MSFT\", start_date=\"2021-01-01\", end_date=\"2021-12-31\")', score=0.0),\n",
       " ModelReturn(model_answer='trading.average_volume(date=\"2024-01-01\", end_date=\"2024-12-31\")', gt_answer='trading.average_volume(symbol=\"AAPL\", start_date=\"2022-01-01\", end_date=\"2022-01-31\")', score=0.0),\n",
       " ModelReturn(model_answer='stock.moving_average(days=30)', gt_answer='stock.moving_average(symbol=\"AAPL\", days=30)', score=1.0),\n",
       " ModelReturn(model_answer='stock.standard_deviation(days=90)', gt_answer='stock.standard_deviation(symbol=\"AAPL\", days=90)', score=1.0),\n",
       " ModelReturn(model_answer='stock.exponential_moving_average(days=50)', gt_answer='stock.exponential_moving_average(symbol=\"AAPL\", days=50)', score=1.0),\n",
       " ModelReturn(model_answer='stock.relative_strength_index(days=14)', gt_answer='stock.relative_strength_index(symbol=\"AAPL\", days=14)', score=1.0),\n",
       " ModelReturn(model_answer='trading.average_volume(date=\"2022-01-01\", end_date=\"2022-06-30\")', gt_answer='trading.average_volume(symbol=\"AAPL\", start_date=\"2021-07-01\", end_date=\"2021-12-31\")', score=0.0),\n",
       " ModelReturn(model_answer='math.sqrt(123)', gt_answer='math.sqrt(number=16)', score=0.0),\n",
       " ModelReturn(model_answer='temperature.celsius_to_fahrenheit', gt_answer='temperature.celsius_to_fahrenheit(celsius=25)', score=0.0),\n",
       " ModelReturn(model_answer='math.factorial(5)', gt_answer='math.factorial(number=5)', score=0.0),\n",
       " ModelReturn(model_answer='random.randint(1, 100)', gt_answer='random.randint(min=1, max=10)', score=0.0),\n",
       " ModelReturn(model_answer='math.exp(1.0)', gt_answer='math.exp(number=2)', score=0.0),\n",
       " ModelReturn(model_answer='math.sin(1.5708)', gt_answer='math.sin(angle=0.5)', score=0.0),\n",
       " ModelReturn(model_answer='math.add(10, 20)', gt_answer='math.add(num1=5, num2=3)', score=0.0),\n",
       " ModelReturn(model_answer='math.log(10)', gt_answer='math.log(number=10, base=2)', score=0.0),\n",
       " ModelReturn(model_answer='statistics.mean(numbers=[1, 2, 3, 4, 5])', gt_answer='statistics.mean(numbers=[2, 4, 6, 8, 10])', score=0.0),\n",
       " ModelReturn(model_answer='game.get_leaderboard(game_id={\"description\": \"ID of the game.\", \"type\": \"integer\"}, sort_by=\"score\", limit=10)', gt_answer=\"game.get_leaderboard(game_id=123, sort_by='score', limit=1)\", score=0.3333333333333333),\n",
       " ModelReturn(model_answer='game.get_weapons()', gt_answer=\"game.get_weapons(game_id=456, type='sword', rarity='legendary')\", score=0.0),\n",
       " ModelReturn(model_answer='game.get_maps()', gt_answer=\"game.get_maps(game_id=789, game_mode='team_deathmatch', player_count=10)\", score=0.0),\n",
       " ModelReturn(model_answer='game.get_quests()', gt_answer=\"game.get_quests(game_id=987, difficulty='medium', reward_type='gold')\", score=0.0),\n",
       " ModelReturn(model_answer='game.get_characters()', gt_answer=\"game.get_characters(game_id=654, combat_style='melee', tier='A')\", score=0.0),\n",
       " ModelReturn(model_answer='meeting.schedule(\"John\", \"Jane\", \"2 PM\")', gt_answer=\"meeting.schedule(participants=['John', 'Jane'], date='2023-10-24', time='2:00 PM')\", score=0.0),\n",
       " ModelReturn(model_answer='meeting.reschedule(new_date=new_date, new_time=new_time)', gt_answer=\"meeting.reschedule(meeting_id='12345', new_date='2023-10-25', new_time='10:00 AM')\", score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer=\"meeting_room.find_available(date='2023-10-24', start_time='1:00 PM', end_time='3:00 PM')\", score=0.0),\n",
       " ModelReturn(model_answer='meeting.invite(date=\"2024-01-01\", time=\"09:00\")', gt_answer=\"meeting.invite(participants=['John', 'Jane', 'Mike', 'Sarah'], date='2023-10-30', time='9:00 AM')\", score=0.0),\n",
       " ModelReturn(model_answer='conference_room.check_availability(date=\"2024-07-26\", time=\"14:00\")', gt_answer=\"conference_room.check_availability(meeting_type='Board Meeting', date='2023-10-27', time='2:00 PM')\", score=0.0),\n",
       " ModelReturn(model_answer='video_conference.schedule(\"2024-07-26 11:00 AM\")', gt_answer=\"video_conference.schedule(participants=['John', 'Jane', 'Mike', 'Sarah'], date='2023-10-26', time='11:00 AM')\", score=0.0),\n",
       " ModelReturn(model_answer='client_meeting.reschedule(new_date=\"2024-08-05\", new_time=\"15:00\")', gt_answer=\"client_meeting.reschedule(meeting_id='67890', new_date='2023-11-05', new_time='3:00 PM')\", score=0.0),\n",
       " ModelReturn(model_answer='meeting_slot.find_available(\"2024-07-26\")', gt_answer=\"meeting_slot.find_available(project='Project X', date='2023-10-31')\", score=0.0),\n",
       " ModelReturn(model_answer='lunch_meeting.schedule(date=\"2024-07-26\", time=\"12:30 PM\")', gt_answer=\"lunch_meeting.schedule(team_members=['John', 'Jane', 'Mike', 'Sarah'], date='2023-10-27', time='12:30 PM')\", score=0.5),\n",
       " ModelReturn(model_answer='tour.book(10, \"Rome\", \"Italy\")', gt_answer='tour.book(destination=\"Rome, Italy\", group_size=10)', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='hiking_trail.find_nearby(location=\"Vancouver, Canada\", criteria=[\"Scenic Views\"])', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='museum.find_nearby(location=\"Paris, France\", criteria=[\"Guided Tours\", \"English Language\"])', score=0.0),\n",
       " ModelReturn(model_answer='resort.find_nearby(amenities={\"All-Inclusive\": True, \"Private Beach\": True, \"Spa\": True, \"Water Sports\": True})', gt_answer='resort.find_nearby(location=\"Cancun, Mexico\", amenities=[\"All-Inclusive\"])', score=0.0),\n",
       " ModelReturn(model_answer='book.get_summary(isbn=\"978-0-000-000000-0\")', gt_answer='book.get_summary(isbn=\"978-3-16-148410-0\")', score=0.0),\n",
       " ModelReturn(model_answer='novels.get_classics(author=\"Jane Austen\")', gt_answer='novels.get_classics(author=\"Jane Austen\")', score=1.0),\n",
       " ModelReturn(model_answer='poems.search(era=\"Romantic\")', gt_answer='poems.search(era=\"Romantic\")', score=1.0),\n",
       " ModelReturn(model_answer='', gt_answer='novels.get_pulitzer_winners(start_year=2011, end_year=2021)', score=0.0),\n",
       " ModelReturn(model_answer='poem.get_author(\"title\")', gt_answer='poem.get_author(title=\"The Raven\")', score=0.0),\n",
       " ModelReturn(model_answer='plays.search(playwright=\"William Shakespeare\")', gt_answer='plays.search(playwright=\"William Shakespeare\")', score=1.0),\n",
       " ModelReturn(model_answer='books.get_best_sellers()', gt_answer='books.get_best_sellers(genre=\"Non-Fiction\")', score=0.0),\n",
       " ModelReturn(model_answer='novel.get_publication_year(\"The Great Gatsby\")', gt_answer='novel.get_publication_year(title=\"To Kill a Mockingbird\")', score=0.0),\n",
       " ModelReturn(model_answer='magazines.search(\"Literary Magazine Search\")', gt_answer='magazines.search(genre=\"Poetry\")', score=0.0),\n",
       " ModelReturn(model_answer='authors.get_nobel_winners()', gt_answer='authors.get_nobel_winners()', score=0.0),\n",
       " ModelReturn(model_answer='population.find(\"Tokyo\", \"Japan\")', gt_answer='population.find(location=\"Tokyo, Japan\")', score=0.0),\n",
       " ModelReturn(model_answer='distance.calculate(\"London,UK\", \"Paris,France\")', gt_answer='distance.calculate(origin=\"London, UK\", destination=\"Paris, France\")', score=0.0),\n",
       " ModelReturn(model_answer='mountain.find_highest()', gt_answer='mountain.find_highest()', score=0.0),\n",
       " ModelReturn(model_answer='capital.find(\"Australia\")', gt_answer='capital.find(country=\"Australia\")', score=0.0),\n",
       " ModelReturn(model_answer='timezone.find(\"New York City\")', gt_answer='timezone.find(location=\"New York City\")', score=0.0),\n",
       " ModelReturn(model_answer='area.calculate(\"Great Barrier Reef\")', gt_answer='area.calculate(location=\"Great Barrier Reef\")', score=0.0),\n",
       " ModelReturn(model_answer='temperature.find_average(location=\"Antarctica\")', gt_answer='temperature.find_average(location=\"Antarctica\")', score=1.0),\n",
       " ModelReturn(model_answer='currency.find(\"Brazil\")', gt_answer='currency.find(country=\"Brazil\")', score=0.0),\n",
       " ModelReturn(model_answer='waterfall.find_highest()', gt_answer='waterfall.find_highest()', score=0.0),\n",
       " ModelReturn(model_answer='language.find_official(\"China\")', gt_answer='language.find_official(country=\"China\")', score=0.0),\n",
       " ModelReturn(model_answer='plant.get_scientific_name(\"Rosa\")', gt_answer='plant.get_scientific_name(common_name=\"rose\")', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='bmi.calculate(weight=70, height=1.75)', score=0.0),\n",
       " ModelReturn(model_answer='population.analyze_eye_color()', gt_answer='population.analyze_eye_color(population=[\"Blue\", \"Brown\", \"Brown\", \"Green\", \"Green\", \"Hazel\", \"Hazel\", \"Hazel\"])', score=0.0),\n",
       " ModelReturn(model_answer='gene.get_sequence(\"gene_id\")', gt_answer='gene.get_sequence(gene_id=\"ABC123\")', score=0.0),\n",
       " ModelReturn(model_answer='genetics.calculate_inheritance_probability(parent1_genotype=\"parent1_genotype\", parent2_genotype=\"parent2_genotype\", trait=\"trait\")', gt_answer='genetics.calculate_inheritance_probability(parent1_genotype=\"AA\", parent2_genotype=\"Aa\", trait=\"Tall\")', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='bird.identify_species(characteristics=[\"Red head\", \"Long beak\"], behaviors=[\"Nesting in trees\", \"Migratory\"])', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='chemistry.calculate_bond_energy(bond_type=\"Covalent\", atoms=[\"H\", \"O\"])', score=0.0),\n",
       " ModelReturn(model_answer='protein.get_sequence(\"protein_id\")', gt_answer='protein.get_sequence(protein_id=\"P12345\")', score=0.0),\n",
       " ModelReturn(model_answer='population.calculate_growth_rate(initial_population=100, final_population=500, time_period=\"1 year\")', gt_answer='population.calculate_growth_rate(initial_population=1000, final_population=1500, time_period=\"10 years\")', score=0.0),\n",
       " ModelReturn(model_answer='taxonomy.identify_classification(organism_name=\"Unknown\")', gt_answer='taxonomy.identify_classification(organism_name=\"Homo sapiens\")', score=0.0)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finetune import evaluation_loop\n",
    "scores = evaluation_loop(test_ds,model,tokenizer,val_batch_size)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'weather.forecast', 'arguments': {'None': ['Paris', 5]}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finetune import parse_python_function_call\n",
    "\n",
    "parse_python_function_call('weather.forecast(Paris, 5)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('test.json', 'r') as file:\n",
    "    test_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Find the highest waterfall in the world.',\n",
       " 'function': {'name': 'Waterfall Finder',\n",
       "  'api_call': 'waterfall.find_highest',\n",
       "  'description': 'Find the highest waterfall in the world.',\n",
       "  'parameters': {'type': 'object', 'properties': {}, 'required': []}},\n",
       " 'model_answer': 'waterfall.find_highest()'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 100\n",
    "test_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'waterfall.find_highest()'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[idx]['model_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "yaml_func = yaml.dump(test_data[idx]['function'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api_call: waterfall.find_highest\n",
      "description: Find the highest waterfall in the world.\n",
      "name: Waterfall Finder\n",
      "parameters:\n",
      "  properties: {}\n",
      "  required: []\n",
      "  type: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(yaml_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finetune\n",
    "idx = 10\n",
    "prompt_messages = finetune._create_messages(\n",
    "    test_data[idx]['question'],\n",
    "    functions=json.dumps(test_data[idx]['function'],indent=1),\n",
    "    # functions=yaml_func,\n",
    "    output=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an expert in composing functions. You are given a question and a set of possible functions. Based on the question, you will need to make one or more function/tool calls to achieve the purpose. If none of the function can be used, point it out. If the given question lacks the parameters required by the function, also point it out. You should only return the function call in tools call sections.\\n'},\n",
       " {'role': 'user',\n",
       "  'content': '#### Question: Find the lyrics of the song \\'Shape of You\\' by Ed Sheeran.Here is a list of functions that you can invoke:\\n{\\n \"name\": \"Lyrics Finder\",\\n \"api_call\": \"lyrics.find\",\\n \"description\": \"Retrieve the lyrics of a specific song.\",\\n \"parameters\": {\\n  \"type\": \"object\",\\n  \"properties\": {\\n   \"song\": {\\n    \"type\": \"string\",\\n    \"description\": \"The name of the song.\"\\n   },\\n   \"artist\": {\\n    \"type\": \"string\",\\n    \"description\": \"The name of the artist.\"\\n   }\\n  },\\n  \"required\": [\\n   \"song\",\\n   \"artist\"\\n  ]\\n }\\n}. Should you decide to return the function call(s), NO other text MUST be included.\\n#### Response:'},\n",
       " {'role': 'assistant', 'content': ''}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = tokenizer.apply_chat_template(\n",
    "    prompt_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "You are an expert in composing functions. You are given a question and a set of possible functions. Based on the question, you will need to make one or more function/tool calls to achieve the purpose. If none of the function can be used, point it out. If the given question lacks the parameters required by the function, also point it out. You should only return the function call in tools call sections.\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "#### Question: Find the lyrics of the song 'Shape of You' by Ed Sheeran.Here is a list of functions that you can invoke:\n",
      "{\n",
      " \"name\": \"Lyrics Finder\",\n",
      " \"api_call\": \"lyrics.find\",\n",
      " \"description\": \"Retrieve the lyrics of a specific song.\",\n",
      " \"parameters\": {\n",
      "  \"type\": \"object\",\n",
      "  \"properties\": {\n",
      "   \"song\": {\n",
      "    \"type\": \"string\",\n",
      "    \"description\": \"The name of the song.\"\n",
      "   },\n",
      "   \"artist\": {\n",
      "    \"type\": \"string\",\n",
      "    \"description\": \"The name of the artist.\"\n",
      "   }\n",
      "  },\n",
      "  \"required\": [\n",
      "   \"song\",\n",
      "   \"artist\"\n",
      "  ]\n",
      " }\n",
      "}. Should you decide to return the function call(s), NO other text MUST be included.\n",
      "#### Response:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.bfloat16, device(type='cuda', index=0))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype, model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.torch_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "unsloth.FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt\n",
    "], return_tensors = \"pt\").to(\"cuda:0\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True, do_sample=False)\n",
    "out = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "You are an expert in composing functions. You are given a question and a set of possible functions. Based on the question, you will need to make one or more function/tool calls to achieve the purpose. If none of the function can be used, point it out. If the given question lacks the parameters required by the function, also point it out. You should only return the function call in tools call sections.\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "#### Question: Find the lyrics of the song 'Shape of You' by Ed Sheeran.Here is a list of functions that you can invoke:\n",
      "{\n",
      " \"name\": \"Lyrics Finder\",\n",
      " \"api_call\": \"lyrics.find\",\n",
      " \"description\": \"Retrieve the lyrics of a specific song.\",\n",
      " \"parameters\": {\n",
      "  \"type\": \"object\",\n",
      "  \"properties\": {\n",
      "   \"song\": {\n",
      "    \"type\": \"string\",\n",
      "    \"description\": \"The name of the song.\"\n",
      "   },\n",
      "   \"artist\": {\n",
      "    \"type\": \"string\",\n",
      "    \"description\": \"The name of the artist.\"\n",
      "   }\n",
      "  },\n",
      "  \"required\": [\n",
      "   \"song\",\n",
      "   \"artist\"\n",
      "  ]\n",
      " }\n",
      "}. Should you decide to return the function call(s), NO other text MUST be included.\n",
      "#### Response:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "lyrics.find(song=\"Shape of You\", artist=\"Ed Sheeran\")<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetune import parse_python_function_call\n",
    "\n",
    "# parse_python_function_call(out[0].)\n",
    "python_output = out[0][out[0].rfind(\"<|end_header_id|>\",1)+19:-10].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lyrics.find(song=\"Shape of You\", artist=\"Ed Sheeran\")'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'lyrics.find', 'arguments': {'song': 'Shape of You', 'artist': 'Ed Sheeran'}}\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def process_ast_node(node):\n",
    "    if isinstance(node, (ast.Constant, ast.Constant, ast.Constant)):\n",
    "        return node.value\n",
    "    elif isinstance(node, ast.List):\n",
    "        return [process_ast_node(elt) for elt in node.elts]\n",
    "    else:\n",
    "        return ast.unparse(node)\n",
    "\n",
    "def parse_python_function_call(call_str):\n",
    "    tree = ast.parse(call_str)\n",
    "    expr = tree.body[0].value\n",
    "\n",
    "    def extract_function_name(node):\n",
    "        if isinstance(node, ast.Name):\n",
    "            return node.id\n",
    "        elif isinstance(node, ast.Attribute):\n",
    "            return f\"{extract_function_name(node.value)}.{node.attr}\"\n",
    "        else:\n",
    "            return ast.unparse(node)\n",
    "    # return expr\n",
    "    function_name = extract_function_name(expr.func)\n",
    "\n",
    "    parameters = {}\n",
    "    noNameParam = []\n",
    "\n",
    "    # Process positional arguments\n",
    "    for arg in expr.args:\n",
    "        noNameParam.append(process_ast_node(arg))\n",
    "\n",
    "    # Process keyword arguments\n",
    "    for kw in expr.keywords:\n",
    "        parameters[kw.arg] = process_ast_node(kw.value)\n",
    "\n",
    "    if noNameParam:\n",
    "        parameters[\"None\"] = noNameParam\n",
    "        \n",
    "    function_dict = {\"name\": function_name, \"arguments\": parameters}\n",
    "    return function_dict\n",
    "\n",
    "# Test the function\n",
    "# call_str = \"gcloud.active-directory.domains.trusts.update(DOMAIN='my-other-domain.com', target_dns_ip_addresses=['10.177.0.3'], target_domain_name='my-target-domain.com')\"\n",
    "result = parse_python_function_call(python_output)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'news.get_headlines', 'arguments': {'source': 'CNN'}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_python_function_call(test_data[11]['model_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "po = 'lyrics.find(song=\"Shape of You\", artist=\"EdSheeran\")'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluation(model_answer,gt_answer):\n",
    "    model_answer = parse_python_function_call(model_answer)\n",
    "    gt_answer = parse_python_function_call(gt_answer)\n",
    "\n",
    "    if model_answer['name'] != gt_answer['name']:\n",
    "        return 0.0\n",
    "    args_score = 0\n",
    "    for model_answer_arg,model_answer_val in model_answer['arguments'].items():\n",
    "        if model_answer_arg not in gt_answer['arguments'] or gt_answer['arguments'][model_answer_arg] != model_answer_val:\n",
    "            args_score+=0\n",
    "        else:\n",
    "            args_score+=1\n",
    "    return args_score/len(model_answer['arguments'])\n",
    "evaluation(python_output,test_data[10]['model_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(po,test_data[10]['model_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = Dataset.from_list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "def _get_tokenized_test_ds(examples,tokenizer,json_or_yaml: Literal[\"json\",\"yaml\"]):\n",
    "    user_prompts = examples['question']\n",
    "    functions = examples['function']\n",
    "    prompts = []\n",
    "    for up,fn in zip(user_prompts,functions):\n",
    "        fn = fn[0] if isinstance(fn,list) else fn\n",
    "        if json_or_yaml == \"json\":\n",
    "            fn = json.dumps(fn,indent=1)\n",
    "        elif json_or_yaml == \"yaml\":\n",
    "            fn = finetune.json_to_yaml(f\"[{fn}]\")\n",
    "        prompts.append(tokenizer.apply_chat_template(\n",
    "            finetune._create_messages(up,fn, \"\"),\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        ))\n",
    "    return {\"prompt\":prompts,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 558.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_ds = test_ds.map(functools.partial(_get_tokenized_test_ds,tokenizer=tokenizer,json_or_yaml=json_or_yaml),batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/recoverx/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/recoverx/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mathe_kunal\u001b[0m (\u001b[33mad-finance\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/recoverx/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mathe_kunal\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/recoverx/astarag/Finetuning/finetuning/finetuning/wandb/run-20241101_192437-zeot376r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/athe_kunal/JSON%20vs%20YAML%20Finetuning%20Project/runs/zeot376r' target=\"_blank\">unsloth/Llama-3.2-1B-Instruct_yaml_bfcl</a></strong> to <a href='https://wandb.ai/athe_kunal/JSON%20vs%20YAML%20Finetuning%20Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/athe_kunal/JSON%20vs%20YAML%20Finetuning%20Project' target=\"_blank\">https://wandb.ai/athe_kunal/JSON%20vs%20YAML%20Finetuning%20Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/athe_kunal/JSON%20vs%20YAML%20Finetuning%20Project/runs/zeot376r' target=\"_blank\">https://wandb.ai/athe_kunal/JSON%20vs%20YAML%20Finetuning%20Project/runs/zeot376r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2024.10.7: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.325 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: We fixed a gradient accumulation bug, but it seems like you don't have the latest transformers version!\n",
      "Please update transformers, TRL and unsloth via:\n",
      "`pip install --upgrade --no-cache-dir --no-deps unsloth transformers git+https://github.com/huggingface/trl.git`\n",
      "Unsloth 2024.10.7 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12125/12125 [00:10<00:00, 1111.01 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 151.21 examples/s]\n",
      "Map (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12125/12125 [00:03<00:00, 3326.67 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12125/12125 [00:02<00:00, 4321.29 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 12,125 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 32 | Total steps = 379\n",
      " \"-____-\"     Number of trainable parameters = 22,544,384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='365' max='379' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [365/379 09:31 < 00:22, 0.64 it/s, Epoch 0.96/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.212100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.775200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.811600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.599200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.338200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.544800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.480100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.545200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.453800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.362100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.716300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.418600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.486400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.337300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.341200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.482200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.542400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.364200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.372000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.508200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.579800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.392600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.353300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.266200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.488600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.359900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.506400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.346500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.345500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.385900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.303200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.571000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.336500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.369100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.330400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.307800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.410800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.404700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.295800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.289500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.298400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.290200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.310200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.305300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.435100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.326200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.420800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.288400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.438200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.403900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.273200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.310600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.378300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.357900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.287100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.305100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.307500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.247700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.471600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.220600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.301100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.231900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.317500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.323500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.470900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.295900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.226900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.318000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.279400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.362200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.309200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.235400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.221600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.406800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.276100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.287700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.335500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.254200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.328200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.309200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.497300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.246500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.251200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.324800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.294300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.393900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.252700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.331400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.403900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.278400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.252500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.216400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.312800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.377200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.277500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.388200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.337600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.254200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.341700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.324800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.219400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.201400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.281400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.294700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.264300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.219800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.438000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.252700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.243400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.351300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.281200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.272300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.258900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.167200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.258800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.199800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.321200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.233000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.176100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.206500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.216100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.239400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.231400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.241600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.234400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.379300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.174400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.360100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.210300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.151700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.245800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.287200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.349500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.232800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.208800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.272700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.246300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.267500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.222100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.344700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.177800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.198900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.176900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.266600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.220700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.347500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.193600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.385100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.248800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.421300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.254200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.251400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.183800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.150400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.167000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.235800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.128800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.260600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.241100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.508400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.257200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.303900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.207400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.239800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.247100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.167900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.222500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.193500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.211900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.224100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.189300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.264400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.235400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.163600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.314900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.188200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.328000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.178500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.325200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.143300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.118200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.149600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.271500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.146900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.157100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.163100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.193300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.180200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.257700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.184500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.169900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.225600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.138200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.286800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.116300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.198800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.130500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.142100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.244100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>0.115300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>0.121600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>0.151200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.164400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>0.245900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>0.183100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.136200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.211500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>0.203800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>0.263000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.181800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>0.131300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.135300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>0.197200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.152200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.116800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>0.111800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.189100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.130500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>0.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>0.134500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>0.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>0.346500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.176500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.103400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.105900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>0.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>0.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.163300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>0.149200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.155900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>0.152100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>0.153000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>0.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>0.160800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.284900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.089900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>0.153500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>0.120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>0.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.116000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>0.147400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>0.144900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>0.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.081300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>0.195300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>0.122900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>0.232400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>0.137600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>0.292700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>0.104900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>0.333800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>0.135500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>0.200200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>0.093600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>0.132900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.157900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>0.106800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>0.098400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>0.116400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.145200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>0.145900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>0.173900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>0.194200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>0.104100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.111800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>0.097800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>0.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>0.152400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>0.138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.130400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.110500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>0.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>0.189700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>0.119900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.131100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>0.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>0.088300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>0.088700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>0.112900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>0.149800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>0.177100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>0.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>0.085200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>0.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.159900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>0.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>0.092400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>0.241300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>0.104500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>0.083500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>0.066900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.142900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>361</td>\n",
       "      <td>0.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362</td>\n",
       "      <td>0.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>363</td>\n",
       "      <td>0.233400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from finetune import run_finetune\n",
    "trainer_stats, trainer, model, tokenizer, scores_returned = run_finetune(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    dataset_name = \"bfcl\",\n",
    "    json_or_yaml = \"yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning-KL8mpKMW-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
