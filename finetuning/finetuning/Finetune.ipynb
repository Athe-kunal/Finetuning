{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 1803.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "import functools\n",
    "from finetune import _get_bfcl_tokenized_test_ds\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#         model_name = \"unsloth/Llama-3.2-1B-Instruct\", # or choose \"unsloth/Llama-3.2-1B\"\n",
    "#         # max_seq_length = max_seq_length,\n",
    "#         dtype = torch.bfloat16,\n",
    "#         load_in_4bit = False,\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "with open('test.json', 'r') as file:\n",
    "    test_data = json.load(file)\n",
    "test_ds = Dataset.from_list(test_data)\n",
    "test_ds = test_ds.map(functools.partial(_get_bfcl_tokenized_test_ds,tokenizer=tokenizer,json_or_yaml=\"json\"),batched=True,remove_columns=[\"function\",\"question\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 742.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_ds = test_ds.map(functools.partial(_get_bfcl_tokenized_test_ds,tokenizer=tokenizer,json_or_yaml=\"json\"),batched=True,remove_columns=[\"function\",\"question\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/recoverx/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/recoverx/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2024.9.post4: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.325 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.9.post4 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12125/12125 [00:01<00:00, 7162.49 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 344.41 examples/s]\n",
      "Map (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12125/12125 [00:03<00:00, 3237.74 examples/s]\n",
      "num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n",
      "Map (num_proc=5): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.24 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12125/12125 [00:03<00:00, 3845.29 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 522.49 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 12,125 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 32 | Total steps = 1\n",
      " \"-____-\"     Number of trainable parameters = 22,544,384\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/1 : < :, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 419, 128256)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'rfind'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfinetune\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_finetune\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun_finetune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munsloth/Llama-3.2-1B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbfcl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson_or_yaml\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/astarag/Finetuning/finetuning/finetuning/finetune.py:100\u001b[0m, in \u001b[0;36mrun_finetune\u001b[0;34m(model_name, dataset_name, json_or_yaml)\u001b[0m\n\u001b[1;32m     61\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m     62\u001b[0m model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m     63\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m ),\n\u001b[1;32m     94\u001b[0m )\n\u001b[1;32m     95\u001b[0m trainer \u001b[38;5;241m=\u001b[39m train_on_responses_only(\n\u001b[1;32m     96\u001b[0m     trainer,\n\u001b[1;32m     97\u001b[0m     instruction_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|start_header_id|>system<|end_header_id|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     98\u001b[0m     response_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|start_header_id|>assistant<|end_header_id|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     99\u001b[0m )\n\u001b[0;32m--> 100\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer_stats, trainer, model, tokenizer\n",
      "File \u001b[0;32m<string>:142\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m<string>:440\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/transformers/trainer.py:2804\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2802\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2804\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   2807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/transformers/trainer.py:2761\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2760\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 2761\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2764\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/transformers/trainer.py:3666\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3663\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3665\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3666\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3667\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3669\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3670\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3674\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3676\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/transformers/trainer.py:3956\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3952\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3953\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[1;32m   3954\u001b[0m         )\n\u001b[1;32m   3955\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3956\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3958\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/astarag/Finetuning/finetuning/finetuning/finetune.py:111\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_preds)\u001b[0m\n\u001b[1;32m    107\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pred, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# Extract the model's answer from between the headers\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     pred_output \u001b[38;5;241m=\u001b[39m pred[\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrfind\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|end_header_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m19\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# Extract ground truth (assuming similar format)\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     gt_output \u001b[38;5;241m=\u001b[39m label[label\u001b[38;5;241m.\u001b[39mrfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|end_header_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m19\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'rfind'"
     ]
    }
   ],
   "source": [
    "from finetune import run_finetune\n",
    "run_finetune(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    dataset_name = \"bfcl\",\n",
    "    json_or_yaml = \"json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/recoverx/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/recoverx/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2024.9.post4: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.325 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.9.post4 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from unsloth.chat_templates import get_chat_template, train_on_responses_only\n",
    "\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "dtype = torch.bfloat16 \n",
    "load_in_4bit = False \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name, # or choose \"unsloth/Llama-3.2-1B\"\n",
    "    # max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                        \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "        lora_alpha = 64,\n",
    "        lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "        bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "        # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "        use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "        random_state = 3407,\n",
    "        use_rslora = False,  # We support rank stabilized LoRA\n",
    "        loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template, train_on_responses_only\n",
    "from datasets import Dataset\n",
    "import functools\n",
    "import json\n",
    "from finetune import _get_bfcl_tokenized_test_ds, _get_bfcl_train_tokenized_ds\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template = \"llama-3.1\",\n",
    "    )\n",
    "train_data = []\n",
    "with open('train.json', 'r') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line.strip())\n",
    "        json_obj['Functions'] = json_obj['Functions'][0] if isinstance(json_obj['Functions'],list) else json_obj['Functions']\n",
    "        json_obj['Output'] = json_obj['Output'][0] if isinstance(json_obj['Output'],list) else json_obj['Output']\n",
    "        train_data.append(json_obj)\n",
    "with open('test.json', 'r') as file:\n",
    "    test_data = json.load(file)\n",
    "# train_data = _process_bfcl_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Instruction': 'Can you provide me with images and videos of a specific location with latitude 40.712776 and longitude -74.005974?\\n',\n",
       " 'Functions': \"{'name': 'RapidAPI', 'api_name': 'requests.get', 'description': 'Geocoding places Info with images & videos.', 'parameters': [{'name': 'lat', 'description': 'Latitude in decimal degrees (wgs84)', 'type': 'NUMBER'}, {'name': 'lng', 'description': 'Longitude in decimal degrees (wgs84)', 'type': 'NUMBER'}, {'name': 'version', 'description': '', 'type': 'string'}, {'name': 'lang', 'description': 'Prefered language of content.', 'type': 'STRING'}]}\\n\",\n",
       " 'Output': 'response = requests.get(\"https://geocoding-places.p.rapidapi.com/get_geocoding_images/v1\", headers={\"X-RapidAPI-Key\": \"SIGN-UP-FOR-KEY\", \"X-RapidAPI-Host\": \"geocoding-places.p.rapidapi.com\"}, params={\"lat\": \"40.712776\", \"lng\": \"-74.005974\", \"version\": \"v1\", \"lang\": \"en\"})'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12125/12125 [00:01<00:00, 7058.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = Dataset.from_list(train_data)\n",
    "json_or_yaml = \"json\"\n",
    "train_ds = train_ds.map(functools.partial(_get_bfcl_train_tokenized_ds,tokenizer=tokenizer,json_or_yaml=json_or_yaml),batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMAINING WORK\n",
    "1. Evaluation for BFCL and xLAM dataset\n",
    "2. finetuning scripts and integration with weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12125/12125 [00:03<00:00, 3240.17 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12125/12125 [00:02<00:00, 4068.64 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 12,125 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 32 | Total steps = 5\n",
      " \"-____-\"     Number of trainable parameters = 22,544,384\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:06, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.783000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.833800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.602900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.350800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_ds,\n",
    "    dataset_text_field = \"prompt\",\n",
    "    max_seq_length = 3072,\n",
    "    dataset_num_proc = 8,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 32,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        # warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 5,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        save_safetensors=True\n",
    "    ),\n",
    ")\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>system<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['model_answer', 'prompt'],\n",
       "    num_rows: 112\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch_size = 16\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "for start in range(0,len(test_ds),val_batch_size):\n",
    "    end = min(len(test_ds),start+val_batch_size)\n",
    "    batch = test_ds[start:end]\n",
    "    prompts = batch['prompt']\n",
    "    inputs = tokenizer(prompts, return_tensors = \"pt\",padding=True,truncation=True,).to(\"cuda:0\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(input_ids = inputs['input_ids'], attention_mask = inputs['attention_mask'], max_new_tokens = 64, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "out = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coffee_shop.find_nearby(amenities={\"Wi-Fi\": true}, location={\"city\": \"San Francisco\", \"state\": \"CA\"})'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][out[0].rfind(\"<|end_header_id|>\",1)+19:].split(\"<|eot_id|>\")[0].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                                                                                                                                                                                                                            \\n\\ngcloud.alpha.anthos.export(\"my-cluster\", \"--project=my-project\", \"--output-directory=my-dir\")<|eot_id|>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[500][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:14<00:00,  2.01s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ModelReturn(model_answer='coffee_shop.find_nearby({\"amenities\": [\"Wi-Fi\"], \"location\": \"San Francisco\"})', gt_answer='coffee_shop.find_nearby(location=\"San Francisco\", amenities=\"Wi-Fi\")', score=0.0),\n",
       " ModelReturn(model_answer='flight.book(\"Los Angeles\", \"New York\", \"June 15th\")', gt_answer='flight.book(origin=\"Los Angeles\", destination=\"New York\", passengers=2, date=\"June 15th\")', score=0.0),\n",
       " ModelReturn(model_answer='restaurant.book_table(date=\"2024-07-26\", time=\"19:00\")', gt_answer='restaurant.book_table(restaurant=\"Italiano\\'s\", location=\"Manhattan, New York\", party_size=2, reservation_time=\"2023-10-23 19:00\")', score=0.0),\n",
       " ModelReturn(model_answer='weather.forecast(Paris, 5)', gt_answer='weather.forecast(location=\"Paris, France\", days=5)', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='pharmacy.find_nearby(location=\"San Diego, California\", feature=\"Drive-thru\")', score=0.0),\n",
       " ModelReturn(model_answer='target.order(items=[{\"items\": {\"description\": \"Ordered items in a list.\", \"type\": \"array\"}, \"type\": \"string\"}, {\"items\": {\"description\": \"Ordered items in a list.\", \"type\": \"array\"}, \"type\": \"string\"}], quantity={\"items\": {\"description\": \"Quantity of the order corresponding to the previous item variable.\", \"type\": \"integer\"}, \"type\": \"array\"}, currency=\"USD\")', gt_answer='target.order(loc=Berkeley,item=[\"potato\", \"chocolate\"], quantity=[6,8])', score=0.0),\n",
       " ModelReturn(model_answer='classes.find(\"Berkeley\", \"Computer Science\", \"undergraduate\")', gt_answer=\"classes.find(school='Berkeley', discipline='computer science', level='undergraduate', status='open')\", score=0.0),\n",
       " ModelReturn(model_answer='gas_station.find_nearby(location={\"description\": \"Current location for the search.\", \"type\": \"string\"})', gt_answer='gas_station.find_nearby(location=\"current\")', score=0.0),\n",
       " ModelReturn(model_answer='restaurant.find_top_rated(amenities={\"Outdoor Seating\": True, \"Vegetarian Options\": True}, location=\"San Francisco\")', gt_answer='restaurant.find_top_rated(location=\"San Francisco\", amenities=[\"Outdoor Seating\"])', score=0.5),\n",
       " ModelReturn(model_answer='timezone.get_current_time()', gt_answer='timezone.get_current_time(timezone=\"America/New_York\")', score=0.0),\n",
       " ModelReturn(model_answer='lyrics.find(\"Shape of You\", \"Ed Sheeran\")', gt_answer='lyrics.find(song=\"Shape of You\", artist=\"Ed Sheeran\")', score=0.0),\n",
       " ModelReturn(model_answer='news.get_headlines(\"CNN\")', gt_answer='news.get_headlines(source=\"CNN\")', score=0.0),\n",
       " ModelReturn(model_answer='dictionary.get_definition(\"serendipity\")', gt_answer='dictionary.get_definition(word=\"serendipity\")', score=0.0),\n",
       " ModelReturn(model_answer='unit_conversion.convert(10, \"miles\", \"kilometers\")', gt_answer='unit_conversion.convert(value=10, from_unit=\"miles\", to_unit=\"kilometers\")', score=0.0),\n",
       " ModelReturn(model_answer='currency_conversion.get_exchange_rate(base_currency=\"USD\", target_currency=\"EUR\")', gt_answer='currency_conversion.get_exchange_rate(base_currency=\"USD\", target_currency=\"EUR\")', score=1.0),\n",
       " ModelReturn(model_answer='movie.search(\"The Shawshank Redemption\")', gt_answer='movie.search(title=\"The Shawshank Redemption\")', score=0.0),\n",
       " ModelReturn(model_answer='stock.get_price(company=\"Apple Inc.\")', gt_answer='stock.get_price(company=\"Apple Inc.\")', score=1.0),\n",
       " ModelReturn(model_answer='park.find_nearby({\"amenities\": \"playground\", \"location\": \"Chicago, IL\"})', gt_answer='park.find_nearby(location=\"Chicago, Illinois\", features=[\"Playground\", \"Picnic Area\"])', score=0.0),\n",
       " ModelReturn(model_answer='translation.translate(text=\"Hello, world!\", target_language=\"Spanish\")', gt_answer='translation.translate(text=\"Hello, world!\", target_language=\"Spanish\")', score=1.0),\n",
       " ModelReturn(model_answer='', gt_answer='trail.find_nearby(location=\"Los Angeles, California\", features=[\"Dog-Friendly\"])', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='store.find_nearby(location=\"San Jose, California\", open_on_sundays=true)', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='restaurant.find_nearby(location=\"Austin, Texas\", cuisine=\"Vegan\", open_for_dinner=true)', score=0.0),\n",
       " ModelReturn(model_answer='timer.set(20, \"Check the oven\")', gt_answer='timer.set(duration=20, message=\"Check the oven\")', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='parking.find_nearby(location=\"Golden Gate Bridge, San Francisco\")', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='book.recommend(interest=\"Science Fiction\")', score=0.0),\n",
       " ModelReturn(model_answer='exchange_rate.get(\"USD\", \"EUR\")', gt_answer='exchange_rate.get(base_currency=\"USD\", target_currency=\"EUR\")', score=0.0),\n",
       " ModelReturn(model_answer='car_rental.search(\"Los Angeles\", \"2024-01-01\", \"2024-12-31\")', gt_answer='car_rental.search(location=\"Los Angeles\", start_date=\"2023-09-01\", end_date=\"2023-09-10\")', score=0.0),\n",
       " ModelReturn(model_answer='timezone.convert(\"Tokyo\", \"Asia/Tokyo\")', gt_answer='timezone.convert(location=\"Tokyo, Japan\", timezone=\"Asia/Tokyo\")', score=0.0),\n",
       " ModelReturn(model_answer='news.search(\"technology\")', gt_answer='news.search(topic=\"technology\")', score=0.0),\n",
       " ModelReturn(model_answer='atm.find_nearby(location={\"latitude\": 37.7749, \"longitude\": -122.4194}, amenities=[\"Cash Deposits\", \"24/7 Access\", \"Wheelchair Access\"], angle=45)', gt_answer='atm.find_nearby(location=\"current_location\", feature=\"Cash Deposits\")', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='hiking_trail.search(location=\"San Francisco\")', score=0.0),\n",
       " ModelReturn(model_answer='exchange_rate.get(\"USD\", \"EUR\")', gt_answer='exchange_rate.get(base_currency=\"USD\", target_currency=\"EUR\")', score=0.0),\n",
       " ModelReturn(model_answer='hotel.search(\"Miami, Florida\", {\"amenities\": [\"Pool\", \"Gym\"], \"type\": \"array\"})', gt_answer='hotel.search(location=\"Miami, Florida\", amenities=[\"Pool\", \"Gym\"])', score=0.0),\n",
       " ModelReturn(model_answer='timezone.convert(\"Tokyo, Japan\", \"America/New_York\")', gt_answer='timezone.convert(location=\"Tokyo, Japan\", target_timezone=\"Asia/Tokyo\")', score=0.0),\n",
       " ModelReturn(model_answer='news.get_headlines(\"CNN\")', gt_answer='news.get_headlines(news_source=\"CNN\")', score=0.0),\n",
       " ModelReturn(model_answer='recipe.search(\"spaghetti carbonara\")', gt_answer='recipe.search(keywords=\"spaghetti carbonara\", dietary_restrictions=[\"Vegetarian\"])', score=0.0),\n",
       " ModelReturn(model_answer='movie_theater.search(\"Los Angeles\", \"blockbuster\")', gt_answer='movie_theater.search(location=\"Los Angeles\", movie_title=\"Avengers: Endgame\")', score=0.0),\n",
       " ModelReturn(model_answer='finance.calculate_compound_interest(principal=5000, interest_rate=0.05, time_period=3)', gt_answer='finance.calculate_compound_interest(principal=5000, interest_rate=0.05, time_period=3)', score=1.0),\n",
       " ModelReturn(model_answer='finance.calculate_mortgage_payment(loan_amount=200000, interest_rate=0.04, loan_term=30)', gt_answer='finance.calculate_mortgage_payment(loan_amount=200000, interest_rate=0.04, loan_term=30)', score=1.0),\n",
       " ModelReturn(model_answer='finance.calculate_roi(investment_amount={\"description\": \"$10,000\", \"type\": \"number\"}, profit={\"description\": \"$2,500\", \"type\": \"number\"})', gt_answer='finance.calculate_roi(investment_amount=10000, profit=2500)', score=0.0),\n",
       " ModelReturn(model_answer='finance.calculate_present_value(future_cash_flow={\"amount\": 5000, \"rate\": 0.08, \"period\": 5}, discount_rate=0.08, time_period=5)', gt_answer='finance.calculate_present_value(future_cash_flow=5000, discount_rate=0.08, time_period=5)', score=0.6666666666666666),\n",
       " ModelReturn(model_answer='finance.calculate_car_loan_payment(loan_amount=20000, interest_rate=0.06, loan_term=5)', gt_answer='finance.calculate_car_loan_payment(loan_amount=20000, interest_rate=0.06, loan_term=5)', score=1.0),\n",
       " ModelReturn(model_answer='finance.calculate_future_value(5000, 0.03, 10)', gt_answer='finance.calculate_future_value(investment_amount=5000, interest_rate=0.03, time_period=10)', score=0.0),\n",
       " ModelReturn(model_answer='finance.calculate_minimum_payment(balance=5000, interest_rate=0.18)', gt_answer='finance.calculate_minimum_payment(balance=5000, interest_rate=0.18)', score=1.0),\n",
       " ModelReturn(model_answer='finance.calculate_roe(net_income=1000000, shareholders_equity=10000000)', gt_answer='finance.calculate_roe(net_income=1000000, shareholders_equity=10000000)', score=1.0),\n",
       " ModelReturn(model_answer='finance.calculate_personal_loan_payment(loan_amount=10000, interest_rate=0.08, loan_term=2)', gt_answer='finance.calculate_personal_loan_payment(loan_amount=10000, interest_rate=0.08, loan_term=2)', score=1.0),\n",
       " ModelReturn(model_answer='stock.average_price(date=\"2024-01-01\")', gt_answer='stock.average_price(symbol=\"AAPL\", start_date=\"2022-01-01\", end_date=\"2022-01-31\")', score=0.0),\n",
       " ModelReturn(model_answer='stock.simulate_walk(days=10)', gt_answer='stock.simulate_walk(symbol=\"AAPL\", days=10)', score=1.0),\n",
       " ModelReturn(model_answer='stock.correlation(\"Apple\", \"Microsoft\", \"2022-01-01\", \"2022-12-31\")', gt_answer='stock.correlation(symbol1=\"AAPL\", symbol2=\"MSFT\", start_date=\"2021-01-01\", end_date=\"2021-12-31\")', score=0.0),\n",
       " ModelReturn(model_answer='trading.average_volume(date=\"2024-01-01\", end_date=\"2024-12-31\")', gt_answer='trading.average_volume(symbol=\"AAPL\", start_date=\"2022-01-01\", end_date=\"2022-01-31\")', score=0.0),\n",
       " ModelReturn(model_answer='stock.moving_average(days=30)', gt_answer='stock.moving_average(symbol=\"AAPL\", days=30)', score=1.0),\n",
       " ModelReturn(model_answer='stock.standard_deviation(days=90)', gt_answer='stock.standard_deviation(symbol=\"AAPL\", days=90)', score=1.0),\n",
       " ModelReturn(model_answer='stock.exponential_moving_average(days=50)', gt_answer='stock.exponential_moving_average(symbol=\"AAPL\", days=50)', score=1.0),\n",
       " ModelReturn(model_answer='stock.relative_strength_index(days=14)', gt_answer='stock.relative_strength_index(symbol=\"AAPL\", days=14)', score=1.0),\n",
       " ModelReturn(model_answer='trading.average_volume(date=\"2022-01-01\", end_date=\"2022-06-30\")', gt_answer='trading.average_volume(symbol=\"AAPL\", start_date=\"2021-07-01\", end_date=\"2021-12-31\")', score=0.0),\n",
       " ModelReturn(model_answer='math.sqrt(123)', gt_answer='math.sqrt(number=16)', score=0.0),\n",
       " ModelReturn(model_answer='temperature.celsius_to_fahrenheit', gt_answer='temperature.celsius_to_fahrenheit(celsius=25)', score=0.0),\n",
       " ModelReturn(model_answer='math.factorial(5)', gt_answer='math.factorial(number=5)', score=0.0),\n",
       " ModelReturn(model_answer='random.randint(1, 100)', gt_answer='random.randint(min=1, max=10)', score=0.0),\n",
       " ModelReturn(model_answer='math.exp(1.0)', gt_answer='math.exp(number=2)', score=0.0),\n",
       " ModelReturn(model_answer='math.sin(1.5708)', gt_answer='math.sin(angle=0.5)', score=0.0),\n",
       " ModelReturn(model_answer='math.add(10, 20)', gt_answer='math.add(num1=5, num2=3)', score=0.0),\n",
       " ModelReturn(model_answer='math.log(10)', gt_answer='math.log(number=10, base=2)', score=0.0),\n",
       " ModelReturn(model_answer='statistics.mean(numbers=[1, 2, 3, 4, 5])', gt_answer='statistics.mean(numbers=[2, 4, 6, 8, 10])', score=0.0),\n",
       " ModelReturn(model_answer='game.get_leaderboard(game_id={\"description\": \"ID of the game.\", \"type\": \"integer\"}, sort_by=\"score\", limit=10)', gt_answer=\"game.get_leaderboard(game_id=123, sort_by='score', limit=1)\", score=0.3333333333333333),\n",
       " ModelReturn(model_answer='game.get_weapons()', gt_answer=\"game.get_weapons(game_id=456, type='sword', rarity='legendary')\", score=0.0),\n",
       " ModelReturn(model_answer='game.get_maps()', gt_answer=\"game.get_maps(game_id=789, game_mode='team_deathmatch', player_count=10)\", score=0.0),\n",
       " ModelReturn(model_answer='game.get_quests()', gt_answer=\"game.get_quests(game_id=987, difficulty='medium', reward_type='gold')\", score=0.0),\n",
       " ModelReturn(model_answer='game.get_characters()', gt_answer=\"game.get_characters(game_id=654, combat_style='melee', tier='A')\", score=0.0),\n",
       " ModelReturn(model_answer='meeting.schedule(\"John\", \"Jane\", \"2 PM\")', gt_answer=\"meeting.schedule(participants=['John', 'Jane'], date='2023-10-24', time='2:00 PM')\", score=0.0),\n",
       " ModelReturn(model_answer='meeting.reschedule(new_date=new_date, new_time=new_time)', gt_answer=\"meeting.reschedule(meeting_id='12345', new_date='2023-10-25', new_time='10:00 AM')\", score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer=\"meeting_room.find_available(date='2023-10-24', start_time='1:00 PM', end_time='3:00 PM')\", score=0.0),\n",
       " ModelReturn(model_answer='meeting.invite(date=\"2024-01-01\", time=\"09:00\")', gt_answer=\"meeting.invite(participants=['John', 'Jane', 'Mike', 'Sarah'], date='2023-10-30', time='9:00 AM')\", score=0.0),\n",
       " ModelReturn(model_answer='conference_room.check_availability(date=\"2024-07-26\", time=\"14:00\")', gt_answer=\"conference_room.check_availability(meeting_type='Board Meeting', date='2023-10-27', time='2:00 PM')\", score=0.0),\n",
       " ModelReturn(model_answer='video_conference.schedule(\"2024-07-26 11:00 AM\")', gt_answer=\"video_conference.schedule(participants=['John', 'Jane', 'Mike', 'Sarah'], date='2023-10-26', time='11:00 AM')\", score=0.0),\n",
       " ModelReturn(model_answer='client_meeting.reschedule(new_date=\"2024-08-05\", new_time=\"15:00\")', gt_answer=\"client_meeting.reschedule(meeting_id='67890', new_date='2023-11-05', new_time='3:00 PM')\", score=0.0),\n",
       " ModelReturn(model_answer='meeting_slot.find_available(\"2024-07-26\")', gt_answer=\"meeting_slot.find_available(project='Project X', date='2023-10-31')\", score=0.0),\n",
       " ModelReturn(model_answer='lunch_meeting.schedule(date=\"2024-07-26\", time=\"12:30 PM\")', gt_answer=\"lunch_meeting.schedule(team_members=['John', 'Jane', 'Mike', 'Sarah'], date='2023-10-27', time='12:30 PM')\", score=0.5),\n",
       " ModelReturn(model_answer='tour.book(10, \"Rome\", \"Italy\")', gt_answer='tour.book(destination=\"Rome, Italy\", group_size=10)', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='hiking_trail.find_nearby(location=\"Vancouver, Canada\", criteria=[\"Scenic Views\"])', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='museum.find_nearby(location=\"Paris, France\", criteria=[\"Guided Tours\", \"English Language\"])', score=0.0),\n",
       " ModelReturn(model_answer='resort.find_nearby(amenities={\"All-Inclusive\": True, \"Private Beach\": True, \"Spa\": True, \"Water Sports\": True})', gt_answer='resort.find_nearby(location=\"Cancun, Mexico\", amenities=[\"All-Inclusive\"])', score=0.0),\n",
       " ModelReturn(model_answer='book.get_summary(isbn=\"978-0-000-000000-0\")', gt_answer='book.get_summary(isbn=\"978-3-16-148410-0\")', score=0.0),\n",
       " ModelReturn(model_answer='novels.get_classics(author=\"Jane Austen\")', gt_answer='novels.get_classics(author=\"Jane Austen\")', score=1.0),\n",
       " ModelReturn(model_answer='poems.search(era=\"Romantic\")', gt_answer='poems.search(era=\"Romantic\")', score=1.0),\n",
       " ModelReturn(model_answer='', gt_answer='novels.get_pulitzer_winners(start_year=2011, end_year=2021)', score=0.0),\n",
       " ModelReturn(model_answer='poem.get_author(\"title\")', gt_answer='poem.get_author(title=\"The Raven\")', score=0.0),\n",
       " ModelReturn(model_answer='plays.search(playwright=\"William Shakespeare\")', gt_answer='plays.search(playwright=\"William Shakespeare\")', score=1.0),\n",
       " ModelReturn(model_answer='books.get_best_sellers()', gt_answer='books.get_best_sellers(genre=\"Non-Fiction\")', score=0.0),\n",
       " ModelReturn(model_answer='novel.get_publication_year(\"The Great Gatsby\")', gt_answer='novel.get_publication_year(title=\"To Kill a Mockingbird\")', score=0.0),\n",
       " ModelReturn(model_answer='magazines.search(\"Literary Magazine Search\")', gt_answer='magazines.search(genre=\"Poetry\")', score=0.0),\n",
       " ModelReturn(model_answer='authors.get_nobel_winners()', gt_answer='authors.get_nobel_winners()', score=0.0),\n",
       " ModelReturn(model_answer='population.find(\"Tokyo\", \"Japan\")', gt_answer='population.find(location=\"Tokyo, Japan\")', score=0.0),\n",
       " ModelReturn(model_answer='distance.calculate(\"London,UK\", \"Paris,France\")', gt_answer='distance.calculate(origin=\"London, UK\", destination=\"Paris, France\")', score=0.0),\n",
       " ModelReturn(model_answer='mountain.find_highest()', gt_answer='mountain.find_highest()', score=0.0),\n",
       " ModelReturn(model_answer='capital.find(\"Australia\")', gt_answer='capital.find(country=\"Australia\")', score=0.0),\n",
       " ModelReturn(model_answer='timezone.find(\"New York City\")', gt_answer='timezone.find(location=\"New York City\")', score=0.0),\n",
       " ModelReturn(model_answer='area.calculate(\"Great Barrier Reef\")', gt_answer='area.calculate(location=\"Great Barrier Reef\")', score=0.0),\n",
       " ModelReturn(model_answer='temperature.find_average(location=\"Antarctica\")', gt_answer='temperature.find_average(location=\"Antarctica\")', score=1.0),\n",
       " ModelReturn(model_answer='currency.find(\"Brazil\")', gt_answer='currency.find(country=\"Brazil\")', score=0.0),\n",
       " ModelReturn(model_answer='waterfall.find_highest()', gt_answer='waterfall.find_highest()', score=0.0),\n",
       " ModelReturn(model_answer='language.find_official(\"China\")', gt_answer='language.find_official(country=\"China\")', score=0.0),\n",
       " ModelReturn(model_answer='plant.get_scientific_name(\"Rosa\")', gt_answer='plant.get_scientific_name(common_name=\"rose\")', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='bmi.calculate(weight=70, height=1.75)', score=0.0),\n",
       " ModelReturn(model_answer='population.analyze_eye_color()', gt_answer='population.analyze_eye_color(population=[\"Blue\", \"Brown\", \"Brown\", \"Green\", \"Green\", \"Hazel\", \"Hazel\", \"Hazel\"])', score=0.0),\n",
       " ModelReturn(model_answer='gene.get_sequence(\"gene_id\")', gt_answer='gene.get_sequence(gene_id=\"ABC123\")', score=0.0),\n",
       " ModelReturn(model_answer='genetics.calculate_inheritance_probability(parent1_genotype=\"parent1_genotype\", parent2_genotype=\"parent2_genotype\", trait=\"trait\")', gt_answer='genetics.calculate_inheritance_probability(parent1_genotype=\"AA\", parent2_genotype=\"Aa\", trait=\"Tall\")', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='bird.identify_species(characteristics=[\"Red head\", \"Long beak\"], behaviors=[\"Nesting in trees\", \"Migratory\"])', score=0.0),\n",
       " ModelReturn(model_answer='', gt_answer='chemistry.calculate_bond_energy(bond_type=\"Covalent\", atoms=[\"H\", \"O\"])', score=0.0),\n",
       " ModelReturn(model_answer='protein.get_sequence(\"protein_id\")', gt_answer='protein.get_sequence(protein_id=\"P12345\")', score=0.0),\n",
       " ModelReturn(model_answer='population.calculate_growth_rate(initial_population=100, final_population=500, time_period=\"1 year\")', gt_answer='population.calculate_growth_rate(initial_population=1000, final_population=1500, time_period=\"10 years\")', score=0.0),\n",
       " ModelReturn(model_answer='taxonomy.identify_classification(organism_name=\"Unknown\")', gt_answer='taxonomy.identify_classification(organism_name=\"Homo sapiens\")', score=0.0)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finetune import evaluation_loop\n",
    "scores = evaluation_loop(test_ds,model,tokenizer,val_batch_size)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'weather.forecast', 'arguments': {'None': ['Paris', 5]}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finetune import parse_python_function_call\n",
    "\n",
    "parse_python_function_call('weather.forecast(Paris, 5)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('test.json', 'r') as file:\n",
    "    test_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Find the highest waterfall in the world.',\n",
       " 'function': {'name': 'Waterfall Finder',\n",
       "  'api_call': 'waterfall.find_highest',\n",
       "  'description': 'Find the highest waterfall in the world.',\n",
       "  'parameters': {'type': 'object', 'properties': {}, 'required': []}},\n",
       " 'model_answer': 'waterfall.find_highest()'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 100\n",
    "test_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'waterfall.find_highest()'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[idx]['model_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "yaml_func = yaml.dump(test_data[idx]['function'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api_call: waterfall.find_highest\n",
      "description: Find the highest waterfall in the world.\n",
      "name: Waterfall Finder\n",
      "parameters:\n",
      "  properties: {}\n",
      "  required: []\n",
      "  type: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(yaml_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finetune\n",
    "idx = 10\n",
    "prompt_messages = finetune._create_messages(\n",
    "    test_data[idx]['question'],\n",
    "    functions=json.dumps(test_data[idx]['function'],indent=1),\n",
    "    # functions=yaml_func,\n",
    "    output=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an expert in composing functions. You are given a question and a set of possible functions. Based on the question, you will need to make one or more function/tool calls to achieve the purpose. If none of the function can be used, point it out. If the given question lacks the parameters required by the function, also point it out. You should only return the function call in tools call sections.\\n'},\n",
       " {'role': 'user',\n",
       "  'content': '#### Question: Find the lyrics of the song \\'Shape of You\\' by Ed Sheeran.Here is a list of functions that you can invoke:\\n{\\n \"name\": \"Lyrics Finder\",\\n \"api_call\": \"lyrics.find\",\\n \"description\": \"Retrieve the lyrics of a specific song.\",\\n \"parameters\": {\\n  \"type\": \"object\",\\n  \"properties\": {\\n   \"song\": {\\n    \"type\": \"string\",\\n    \"description\": \"The name of the song.\"\\n   },\\n   \"artist\": {\\n    \"type\": \"string\",\\n    \"description\": \"The name of the artist.\"\\n   }\\n  },\\n  \"required\": [\\n   \"song\",\\n   \"artist\"\\n  ]\\n }\\n}. Should you decide to return the function call(s), NO other text MUST be included.\\n#### Response:'},\n",
       " {'role': 'assistant', 'content': ''}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = tokenizer.apply_chat_template(\n",
    "    prompt_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "You are an expert in composing functions. You are given a question and a set of possible functions. Based on the question, you will need to make one or more function/tool calls to achieve the purpose. If none of the function can be used, point it out. If the given question lacks the parameters required by the function, also point it out. You should only return the function call in tools call sections.\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "#### Question: Find the lyrics of the song 'Shape of You' by Ed Sheeran.Here is a list of functions that you can invoke:\n",
      "{\n",
      " \"name\": \"Lyrics Finder\",\n",
      " \"api_call\": \"lyrics.find\",\n",
      " \"description\": \"Retrieve the lyrics of a specific song.\",\n",
      " \"parameters\": {\n",
      "  \"type\": \"object\",\n",
      "  \"properties\": {\n",
      "   \"song\": {\n",
      "    \"type\": \"string\",\n",
      "    \"description\": \"The name of the song.\"\n",
      "   },\n",
      "   \"artist\": {\n",
      "    \"type\": \"string\",\n",
      "    \"description\": \"The name of the artist.\"\n",
      "   }\n",
      "  },\n",
      "  \"required\": [\n",
      "   \"song\",\n",
      "   \"artist\"\n",
      "  ]\n",
      " }\n",
      "}. Should you decide to return the function call(s), NO other text MUST be included.\n",
      "#### Response:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.bfloat16, device(type='cuda', index=0))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype, model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.torch_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "unsloth.FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt\n",
    "], return_tensors = \"pt\").to(\"cuda:0\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True, do_sample=False)\n",
    "out = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "You are an expert in composing functions. You are given a question and a set of possible functions. Based on the question, you will need to make one or more function/tool calls to achieve the purpose. If none of the function can be used, point it out. If the given question lacks the parameters required by the function, also point it out. You should only return the function call in tools call sections.\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "#### Question: Find the lyrics of the song 'Shape of You' by Ed Sheeran.Here is a list of functions that you can invoke:\n",
      "{\n",
      " \"name\": \"Lyrics Finder\",\n",
      " \"api_call\": \"lyrics.find\",\n",
      " \"description\": \"Retrieve the lyrics of a specific song.\",\n",
      " \"parameters\": {\n",
      "  \"type\": \"object\",\n",
      "  \"properties\": {\n",
      "   \"song\": {\n",
      "    \"type\": \"string\",\n",
      "    \"description\": \"The name of the song.\"\n",
      "   },\n",
      "   \"artist\": {\n",
      "    \"type\": \"string\",\n",
      "    \"description\": \"The name of the artist.\"\n",
      "   }\n",
      "  },\n",
      "  \"required\": [\n",
      "   \"song\",\n",
      "   \"artist\"\n",
      "  ]\n",
      " }\n",
      "}. Should you decide to return the function call(s), NO other text MUST be included.\n",
      "#### Response:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "lyrics.find(song=\"Shape of You\", artist=\"Ed Sheeran\")<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetune import parse_python_function_call\n",
    "\n",
    "# parse_python_function_call(out[0].)\n",
    "python_output = out[0][out[0].rfind(\"<|end_header_id|>\",1)+19:-10].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lyrics.find(song=\"Shape of You\", artist=\"Ed Sheeran\")'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'lyrics.find', 'arguments': {'song': 'Shape of You', 'artist': 'Ed Sheeran'}}\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def process_ast_node(node):\n",
    "    if isinstance(node, (ast.Constant, ast.Constant, ast.Constant)):\n",
    "        return node.value\n",
    "    elif isinstance(node, ast.List):\n",
    "        return [process_ast_node(elt) for elt in node.elts]\n",
    "    else:\n",
    "        return ast.unparse(node)\n",
    "\n",
    "def parse_python_function_call(call_str):\n",
    "    tree = ast.parse(call_str)\n",
    "    expr = tree.body[0].value\n",
    "\n",
    "    def extract_function_name(node):\n",
    "        if isinstance(node, ast.Name):\n",
    "            return node.id\n",
    "        elif isinstance(node, ast.Attribute):\n",
    "            return f\"{extract_function_name(node.value)}.{node.attr}\"\n",
    "        else:\n",
    "            return ast.unparse(node)\n",
    "    # return expr\n",
    "    function_name = extract_function_name(expr.func)\n",
    "\n",
    "    parameters = {}\n",
    "    noNameParam = []\n",
    "\n",
    "    # Process positional arguments\n",
    "    for arg in expr.args:\n",
    "        noNameParam.append(process_ast_node(arg))\n",
    "\n",
    "    # Process keyword arguments\n",
    "    for kw in expr.keywords:\n",
    "        parameters[kw.arg] = process_ast_node(kw.value)\n",
    "\n",
    "    if noNameParam:\n",
    "        parameters[\"None\"] = noNameParam\n",
    "        \n",
    "    function_dict = {\"name\": function_name, \"arguments\": parameters}\n",
    "    return function_dict\n",
    "\n",
    "# Test the function\n",
    "# call_str = \"gcloud.active-directory.domains.trusts.update(DOMAIN='my-other-domain.com', target_dns_ip_addresses=['10.177.0.3'], target_domain_name='my-target-domain.com')\"\n",
    "result = parse_python_function_call(python_output)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'news.get_headlines', 'arguments': {'source': 'CNN'}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_python_function_call(test_data[11]['model_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "po = 'lyrics.find(song=\"Shape of You\", artist=\"EdSheeran\")'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluation(model_answer,gt_answer):\n",
    "    model_answer = parse_python_function_call(model_answer)\n",
    "    gt_answer = parse_python_function_call(gt_answer)\n",
    "\n",
    "    if model_answer['name'] != gt_answer['name']:\n",
    "        return 0.0\n",
    "    args_score = 0\n",
    "    for model_answer_arg,model_answer_val in model_answer['arguments'].items():\n",
    "        if model_answer_arg not in gt_answer['arguments'] or gt_answer['arguments'][model_answer_arg] != model_answer_val:\n",
    "            args_score+=0\n",
    "        else:\n",
    "            args_score+=1\n",
    "    return args_score/len(model_answer['arguments'])\n",
    "evaluation(python_output,test_data[10]['model_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(po,test_data[10]['model_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = Dataset.from_list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "def _get_tokenized_test_ds(examples,tokenizer,json_or_yaml: Literal[\"json\",\"yaml\"]):\n",
    "    user_prompts = examples['question']\n",
    "    functions = examples['function']\n",
    "    prompts = []\n",
    "    for up,fn in zip(user_prompts,functions):\n",
    "        fn = fn[0] if isinstance(fn,list) else fn\n",
    "        if json_or_yaml == \"json\":\n",
    "            fn = json.dumps(fn,indent=1)\n",
    "        elif json_or_yaml == \"yaml\":\n",
    "            fn = finetune.json_to_yaml(f\"[{fn}]\")\n",
    "        prompts.append(tokenizer.apply_chat_template(\n",
    "            finetune._create_messages(up,fn, \"\"),\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        ))\n",
    "    return {\"prompt\":prompts,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 558.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_ds = test_ds.map(functools.partial(_get_tokenized_test_ds,tokenizer=tokenizer,json_or_yaml=json_or_yaml),batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5064"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = \"\"\"\n",
    "<|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|finetune_right_pad_id|><|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 26 July 2024\n",
    "\n",
    "You are an expert in composing functions. You are given a question and a set of possible functions. Based on the question, you will need to make one or more function/tool calls to achieve the purpose. If none of the function can be used, point it out. If the given question lacks the parameters required by the function, also point it out. You should only return the function call in tools call sections.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "#### Question: Find a coffee shop near me with free Wi-Fi in San Francisco.Here is a list of functions that you can invoke:\n",
    "{\n",
    " \"api_call\": \"coffee_shop.find_nearby\",\n",
    " \"description\": \"Locate nearby coffee shops based on specific criteria like Wi-Fi availability.\",\n",
    " \"name\": \"Coffee Shop Locator\",\n",
    " \"parameters\": {\n",
    "  \"properties\": {\n",
    "   \"amenities\": {\n",
    "    \"description\": \"Preferred amenities.\",\n",
    "    \"items\": {\n",
    "     \"enum\": [\n",
    "      \"Wi-Fi\",\n",
    "      \"Outdoor Seating\",\n",
    "      \"Bakery\",\n",
    "      \"Vegetarian Options\"\n",
    "     ],\n",
    "     \"type\": \"string\"\n",
    "    },\n",
    "    \"type\": \"array\"\n",
    "   },\n",
    "   \"angle\": null,\n",
    "   \"artist\": null,\n",
    "   \"atoms\": null,\n",
    "   \"author\": null,\n",
    "   \"balance\": null,\n",
    "   \"base\": null,\n",
    "   \"base_currency\": null,\n",
    "   \"behaviors\": null,\n",
    "   \"bond_type\": null,\n",
    "   \"category\": null,\n",
    "   \"celsius\": null,\n",
    "   \"characteristics\": null,\n",
    "   \"combat_style\": null,\n",
    "   \"common_name\": null,\n",
    "   \"company\": null,\n",
    "   \"country\": null,\n",
    "   \"criteria\": null,\n",
    "   \"cuisine\": null,\n",
    "   \"date\": null,\n",
    "   \"days\": null,\n",
    "   \"destination\": null,\n",
    "   \"dietary_restrictions\": null,\n",
    "   \"difficulty\": null,\n",
    "   \"discipline\": null,\n",
    "   \"discount_rate\": null,\n",
    "   \"duration\": null,\n",
    "   \"end_date\": null,\n",
    "   \"end_time\": null,\n",
    "   \"end_year\": null,\n",
    "   \"era\": null,\n",
    "   \"feature\": null,\n",
    "   \"features\": null,\n",
    "   \"final_population\": null,\n",
    "   \"from_unit\": null,\n",
    "   \"future_cash_flow\": null,\n",
    "   \"game_id\": null,\n",
    "   \"game_mode\": null,\n",
    "   \"gene_id\": null,\n",
    "   \"genre\": null,\n",
    "   \"group_size\": null,\n",
    "   \"height\": null,\n",
    "   \"initial_population\": null,\n",
    "   \"interest\": null,\n",
    "   \"interest_rate\": null,\n",
    "   \"investment_amount\": null,\n",
    "   \"isbn\": null,\n",
    "   \"item\": null,\n",
    "   \"keywords\": null,\n",
    "   \"level\": null,\n",
    "   \"limit\": null,\n",
    "   \"loan_amount\": null,\n",
    "   \"loan_term\": null,\n",
    "   \"loc\": null,\n",
    "   \"location\": {\n",
    "    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "    \"type\": \"string\"\n",
    "   },\n",
    "   \"max\": null,\n",
    "   \"meeting_id\": null,\n",
    "   \"meeting_type\": null,\n",
    "   \"message\": null,\n",
    "   \"min\": null,\n",
    "   \"movie_title\": null,\n",
    "   \"net_income\": null,\n",
    "   \"new_date\": null,\n",
    "   \"new_time\": null,\n",
    "   \"news_source\": null,\n",
    "   \"num1\": null,\n",
    "   \"num2\": null,\n",
    "   \"number\": null,\n",
    "   \"numbers\": null,\n",
    "   \"open_for_dinner\": null,\n",
    "   \"open_on_sundays\": null,\n",
    "   \"organism_name\": null,\n",
    "   \"origin\": null,\n",
    "   \"parent1_genotype\": null,\n",
    "   \"parent2_genotype\": null,\n",
    "   \"participants\": null,\n",
    "   \"party_size\": null,\n",
    "   \"passengers\": null,\n",
    "   \"player_count\": null,\n",
    "   \"playwright\": null,\n",
    "   \"population\": null,\n",
    "   \"principal\": null,\n",
    "   \"profit\": null,\n",
    "   \"project\": null,\n",
    "   \"protein_id\": null,\n",
    "   \"quantity\": null,\n",
    "   \"rarity\": null,\n",
    "   \"reservation_time\": null,\n",
    "   \"restaurant\": null,\n",
    "   \"reward_type\": null,\n",
    "   \"school\": null,\n",
    "   \"shareholders_equity\": null,\n",
    "   \"song\": null,\n",
    "   \"sort_by\": null,\n",
    "   \"source\": null,\n",
    "   \"start_date\": null,\n",
    "   \"start_time\": null,\n",
    "   \"start_year\": null,\n",
    "   \"status\": null,\n",
    "   \"symbol\": null,\n",
    "   \"symbol1\": null,\n",
    "   \"symbol2\": null,\n",
    "   \"target_currency\": null,\n",
    "   \"target_language\": null,\n",
    "   \"target_timezone\": null,\n",
    "   \"team_members\": null,\n",
    "   \"text\": null,\n",
    "   \"tier\": null,\n",
    "   \"time\": null,\n",
    "   \"time_period\": null,\n",
    "   \"timezone\": null,\n",
    "   \"title\": null,\n",
    "   \"to_unit\": null,\n",
    "   \"topic\": null,\n",
    "   \"trait\": null,\n",
    "   \"type\": null,\n",
    "   \"value\": null,\n",
    "   \"weight\": null,\n",
    "   \"word\": null\n",
    "  },\n",
    "  \"required\": [\n",
    "   \"location\"\n",
    "  ],\n",
    "  \"type\": \"object\"\n",
    " },\n",
    " \"required\": null\n",
    "}. Should you decide to return the function call(s), NO other text MUST be included.\n",
    "#### Response:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "coffee_shop.find_nearby({\"amenities\": [\"Wi-Fi\"], \"location\": \"San Francisco\"})<|eot_id|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "s.rindex(\"<|end_header_id|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/recoverx/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/recoverx/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mathe_kunal\u001b[0m (\u001b[33mad-finance\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/recoverx/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mathe_kunal\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/recoverx/astarag/Finetuning/finetuning/finetuning/wandb/run-20241101_024757-bv90gqnf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/athe_kunal/JSON%20vs%20YAML%20Finetuning%20Project/runs/bv90gqnf' target=\"_blank\">unsloth/Llama-3.2-1B-Instruct_json_bfcl</a></strong> to <a href='https://wandb.ai/athe_kunal/JSON%20vs%20YAML%20Finetuning%20Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/athe_kunal/JSON%20vs%20YAML%20Finetuning%20Project' target=\"_blank\">https://wandb.ai/athe_kunal/JSON%20vs%20YAML%20Finetuning%20Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/athe_kunal/JSON%20vs%20YAML%20Finetuning%20Project/runs/bv90gqnf' target=\"_blank\">https://wandb.ai/athe_kunal/JSON%20vs%20YAML%20Finetuning%20Project/runs/bv90gqnf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2024.10.7: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.325 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: We fixed a gradient accumulation bug, but it seems like you don't have the latest transformers version!\n",
      "Please update transformers, TRL and unsloth via:\n",
      "`pip install --upgrade --no-cache-dir --no-deps unsloth transformers git+https://github.com/huggingface/trl.git`\n",
      "Unsloth 2024.10.7 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12125/12125 [00:02<00:00, 5168.48 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 705.05 examples/s]\n",
      "Map (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12125/12125 [00:03<00:00, 3778.05 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12125/12125 [00:03<00:00, 3577.40 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 12,125 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 32 | Total steps = 1\n",
      " \"-____-\"     Number of trainable parameters = 22,544,384\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.239000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:11<00:00,  2.79s/it]\n"
     ]
    }
   ],
   "source": [
    "from finetune import run_finetune\n",
    "trainer_stats, trainer, model, tokenizer, scores_returned = run_finetune(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    dataset_name = \"bfcl\",\n",
    "    json_or_yaml = \"json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning-KL8mpKMW-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
