{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/recoverx/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/recoverx/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "print(os.environ['CUDA_VISIBLE_DEVICES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"Salesforce/xlam-function-calling-60k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "tools = ast.literal_eval(ds['train'][10]['tools'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = ds['train'][10]['answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'poor_backlinks', 'arguments': {'domain': 'example.com'}},\n",
       " {'name': 'qrcode', 'arguments': {'data': 'Visit our website at example.com'}}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Where can I find live giveaways for beta access and games?',\n",
       " 'id': 0,\n",
       " 'answers': '[{\"name\": \"live_giveaways_by_type\", \"arguments\": {\"type\": \"beta\"}}, {\"name\": \"live_giveaways_by_type\", \"arguments\": {\"type\": \"game\"}}]',\n",
       " 'tools': '[{\"name\": \"live_giveaways_by_type\", \"description\": \"Retrieve live giveaways from the GamerPower API based on the specified type.\", \"parameters\": {\"type\": {\"description\": \"The type of giveaways to retrieve (e.g., game, loot, beta).\", \"type\": \"str\", \"default\": \"game\"}}}]'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"name\": \"live_giveaways_by_type\", \"description\": \"Retrieve live giveaways from the GamerPower API based on the specified type.\", \"parameters\": {\"type\": {\"description\": \"The type of giveaways to retrieve (e.g., game, loot, beta).\", \"type\": \"str\", \"default\": \"game\"}}}'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]['tools'][1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'live_giveaways_by_type(type=beta)'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = ast.literal_eval(ds['train'][0]['answers'])\n",
    "def _convert_answer(answer):\n",
    "    python_output = answer['name'] +\"(\"\n",
    "    for k,v in answer['arguments'].items():\n",
    "        python_output += f\"{k}={v},\"\n",
    "    python_output = python_output[:-1]\n",
    "    python_output += \")\"\n",
    "    return python_output\n",
    "_convert_answer(answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'live_giveaways_by_type', 'arguments': {'type': 'beta'}},\n",
       " {'name': 'live_giveaways_by_type', 'arguments': {'type': 'game'}}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2024.10.7: Fast Llama patching. Transformers = 4.46.1.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.325 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/recoverx/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/accelerate/utils/modeling.py:1462: UserWarning: Current model requires 142607360 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "could not get source code",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16 \n\u001b[1;32m      5\u001b[0m load_in_4bit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \n\u001b[0;32m----> 6\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munsloth/Llama-3.2-1B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# max_seq_length = max_seq_length,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m get_chat_template(\n\u001b[1;32m     14\u001b[0m         tokenizer,\n\u001b[1;32m     15\u001b[0m         chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/unsloth/models/loader.py:332\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, *args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m     tokenizer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(resize_model_vocab)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/unsloth/models/llama.py:1790\u001b[0m, in \u001b[0;36mFastLlamaModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[1;32m   1787\u001b[0m Trainer\u001b[38;5;241m.\u001b[39m_inner_training_loop \u001b[38;5;241m=\u001b[39m _fast_inner_training_loop\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;66;03m# Fix gradient accumulation\u001b[39;00m\n\u001b[0;32m-> 1790\u001b[0m \u001b[43mpatch_gradient_accumulation_fix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;66;03m# Save tokenizer for inference purposes\u001b[39;00m\n\u001b[1;32m   1793\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Force inference\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/unsloth/models/_utils.py:1220\u001b[0m, in \u001b[0;36mpatch_gradient_accumulation_fix\u001b[0;34m(Trainer)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;66;03m# Also fix up loss scaling ie negate loss *= self.args.gradient_accumulation_steps\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(Trainer\u001b[38;5;241m.\u001b[39mtraining_step)\u001b[38;5;241m.\u001b[39mparameters: \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1220\u001b[0m function \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetsource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1221\u001b[0m where \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdef\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1222\u001b[0m function \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/inspect.py:1147\u001b[0m, in \u001b[0;36mgetsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetsource\u001b[39m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the text of the source code for an object.\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \n\u001b[1;32m   1144\u001b[0m \u001b[38;5;124;03m    The argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;124;03m    or code object.  The source code is returned as a single string.  An\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;124;03m    OSError is raised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1147\u001b[0m     lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mgetsourcelines\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lines)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/inspect.py:1129\u001b[0m, in \u001b[0;36mgetsourcelines\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of source lines and starting line number for an object.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \n\u001b[1;32m   1123\u001b[0m \u001b[38;5;124;03mThe argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;124;03moriginal source file the first line of code was found.  An OSError is\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;124;03mraised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;241m=\u001b[39m unwrap(\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m-> 1129\u001b[0m lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m istraceback(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28mobject\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39mtb_frame\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/inspect.py:958\u001b[0m, in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    956\u001b[0m     lines \u001b[38;5;241m=\u001b[39m linecache\u001b[38;5;241m.\u001b[39mgetlines(file)\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lines:\n\u001b[0;32m--> 958\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcould not get source code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ismodule(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lines, \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mOSError\u001b[0m: could not get source code"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, get_chat_template\n",
    "import torch\n",
    "\n",
    "dtype = torch.bfloat16 \n",
    "load_in_4bit = False \n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"unsloth/Llama-3.2-1B-Instruct\",\n",
    "        # max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template = \"llama-3.1\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Where can I find live giveaways for beta access and games?',\n",
       " 'id': 0,\n",
       " 'answers': '[{\"name\": \"live_giveaways_by_type\", \"arguments\": {\"type\": \"beta\"}}, {\"name\": \"live_giveaways_by_type\", \"arguments\": {\"type\": \"game\"}}]',\n",
       " 'tools': '[{\"name\": \"live_giveaways_by_type\", \"description\": \"Retrieve live giveaways from the GamerPower API based on the specified type.\", \"parameters\": {\"type\": {\"description\": \"The type of giveaways to retrieve (e.g., game, loot, beta).\", \"type\": \"str\", \"default\": \"game\"}}}]'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "import yaml\n",
    "import sys\n",
    "import json\n",
    "import ast\n",
    "\n",
    "def clean_string(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans a string by encoding it to UTF-8 and replacing unencodable characters.\n",
    "    \"\"\"\n",
    "    return s.encode('utf-8', errors='replace').decode('utf-8')\n",
    "\n",
    "def remove_malinformed_str(data):\n",
    "    data = data.replace(\"true\", \"True\")\n",
    "    data = data.replace(\"false\", \"False\")\n",
    "    data = data.replace(\"null\", \"None\")\n",
    "    return data\n",
    "def process_xlam_data(example_list, json_or_yaml: Literal['json', 'yaml']):\n",
    "    prompts = []\n",
    "    queries = example_list['query']\n",
    "    answers = example_list['answers']\n",
    "    tools = example_list['tools']\n",
    "    \n",
    "    for i in range(len(queries)):\n",
    "        try:\n",
    "            try:\n",
    "                # Clean the query string\n",
    "                queries[i] = clean_string(queries[i])\n",
    "                \n",
    "                # Replace JSON literals with Python literals\n",
    "                \n",
    "                answers[i] = remove_malinformed_str(answers[i])\n",
    "                tools[i] = remove_malinformed_str(tools[i])\n",
    "                # Convert the first answer entry\n",
    "                answer = _convert_answer(ast.literal_eval(answers[i])[0])\n",
    "                answer = clean_string(answer)  # Clean the converted answer\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing answers[{i}]: {e}\")\n",
    "                print(\"Error Content:\", answers[i])\n",
    "                continue  # Skip to the next iteration if there's an error\n",
    "            \n",
    "            # Process tools based on the specified format\n",
    "            if json_or_yaml == 'json':\n",
    "                functions = json.dumps(ast.literal_eval(tools[i][1:-1]),indent=1)\n",
    "            elif json_or_yaml == 'yaml':\n",
    "                functions = json_to_yaml(tools[i])\n",
    "            functions = clean_string(functions)  # Clean the functions string\n",
    "            \n",
    "            # Create and clean the message\n",
    "            messages = _create_messages(queries[i], functions, answer)\n",
    "            messages = sanitize_messages(messages)  # Clean all message contents\n",
    "            \n",
    "            # Append the processed prompt\n",
    "            prompts.append(\n",
    "                tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "            )\n",
    "        except (UnicodeEncodeError, UnicodeDecodeError) as e:\n",
    "            print(f\"Unicode error at example {i}: {e}\")\n",
    "            continue  # Skip examples that cause encoding errors\n",
    "    \n",
    "    return {\"prompt\": prompts}\n",
    "\n",
    "def _convert_answer(answer):\n",
    "    # Existing implementation\n",
    "    python_output = answer['name'] + \"(\"\n",
    "    for k, v in answer['arguments'].items():\n",
    "        python_output += f\"{k}={v},\"\n",
    "    python_output = python_output[:-1]\n",
    "    python_output += \")\"\n",
    "    return python_output \n",
    "\n",
    "def json_to_yaml(data):\n",
    "    curr_func_yaml = \"\"\n",
    "    json_func = ast.literal_eval(data)\n",
    "    for func in json_func:\n",
    "        curr_func_yaml += yaml.dump(func) + \"\\n\\n\"\n",
    "    return curr_func_yaml\n",
    "\n",
    "def _create_messages(user_prompt: str, functions: str, output: str):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an expert in composing functions. You are given a question and a set of possible functions.\"\n",
    "                \" Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\"\n",
    "                \" If none of the functions can be used, point it out. If the given question lacks the parameters required by the function, also point it out.\"\n",
    "                \" You should only return the function call in tools call sections.\\n\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"#### Question: {user_prompt} \"\n",
    "                f\"Here is a list of functions in JSON or YAML format that you can invoke:\\n{functions}. \"\n",
    "                \"Should you decide to return the function call(s), NO other text MUST be included.\\n\"\n",
    "                \"#### Response:\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": output\n",
    "        }\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "def sanitize_messages(messages):\n",
    "    \"\"\"\n",
    "    Sanitizes all 'content' fields in the messages to ensure they are UTF-8 compliant.\n",
    "    \"\"\"\n",
    "    for message in messages:\n",
    "        if 'content' in message:\n",
    "            message['content'] = clean_string(message['content'])\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [01:05<00:00, 921.45 examples/s] \n"
     ]
    }
   ],
   "source": [
    "tokenized_ds = ds.map(process_xlam_data,batched=True,fn_kwargs={'json_or_yaml':'json'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [00:00<00:00, 153290.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds.save_to_disk(\"xlam_data_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [07:02<00:00, 142.05 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [00:00<00:00, 320503.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds = ds.map(process_xlam_data,batched=True,fn_kwargs={'json_or_yaml':'yaml'})\n",
    "tokenized_ds.save_to_disk(\"xlam_data_yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'poor_backlinks',\n",
       " 'description': 'Fetch poor quality backlinks for a given domain using the Best Backlink Checker API.',\n",
       " 'parameters': {'domain': {'description': 'The domain for which to fetch the poor quality backlinks.',\n",
       "   'type': 'str',\n",
       "   'default': 'getecz.com'}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "ast.literal_eval(ds['train'][10]['tools'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"name\": \"live_giveaways_by_type\", \"arguments\": {\"type\": \"beta\"}}, {\"name\": \"live_giveaways_by_type\", \"arguments\": {\"type\": \"game\"}}]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]['answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'func', 'arguments': {'a': 4, 'b': 5, 'None': [1, [1, 2], 3]}}\n",
      "{'name': 'func', 'arguments': {'x': 1, 'b': '2', 'c': [1, 2, {'a': 1, 'b': 2}], 'None': ['cde']}}\n",
      "{'name': 'get_current_weather', 'arguments': {'location': 'Boston, MA', 'api_key': 123456789, 'unit': 'fahrenheit'}}\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def process_ast_node(node):\n",
    "    # Check if the node is a function call\n",
    "    if isinstance(node, ast.Call):\n",
    "        # Return a string representation of the function call\n",
    "        return ast.unparse(node) \n",
    "    else:\n",
    "        # Convert the node to source code and evaluate to get the value\n",
    "        node_str = ast.unparse(node)\n",
    "        return eval(node_str)\n",
    "\n",
    "        \n",
    "def parse_python_function_call(call_str):\n",
    "    tree = ast.parse(call_str)\n",
    "    expr = tree.body[0]\n",
    "\n",
    "    call_node = expr.value\n",
    "    function_name = (\n",
    "        call_node.func.id\n",
    "        if isinstance(call_node.func, ast.Name)\n",
    "        else str(call_node.func)\n",
    "    )\n",
    "\n",
    "    parameters = {}\n",
    "    noNameParam = []\n",
    "\n",
    "    # Process positional arguments\n",
    "    for arg in call_node.args:\n",
    "        noNameParam.append(process_ast_node(arg))\n",
    "\n",
    "    # Process keyword arguments\n",
    "    for kw in call_node.keywords:\n",
    "        parameters[kw.arg] = process_ast_node(kw.value)\n",
    "\n",
    "    if noNameParam:\n",
    "        parameters[\"None\"] = noNameParam\n",
    "        \n",
    "    function_dict = {\"name\": function_name, \"arguments\": parameters}\n",
    "    return function_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    call_str = \"func(1, [1, 2], 3, a=4, b=5)\"\n",
    "    print(parse_python_function_call(call_str))\n",
    "\n",
    "    call_str = \"func('cde', x=1, b='2', c=[1, 2, {'a': 1, 'b': 2}])\"\n",
    "    print(parse_python_function_call(call_str))\n",
    "\n",
    "    call_str = \"get_current_weather(location='Boston, MA', api_key=123456789, unit='fahrenheit')\"\n",
    "    print(parse_python_function_call(call_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/recoverx/.cache/pypoetry/virtualenvs/finetuning-KL8mpKMW-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78577/78577 [00:00<00:00, 244032.27 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12500/12500 [00:01<00:00, 9794.84 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_1637981_7 (detroit__dtw_ VARCHAR, grand_rapids__grr_ VARCHAR)', 'role': 'system'}, {'content': \"When Grand Rapids's fare was $377.29, what is the fare to Detroit?\", 'role': 'user'}, {'content': 'SELECT detroit__dtw_ FROM table_1637981_7 WHERE grand_rapids__grr_ = \"$377.29\"', 'role': 'assistant'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    " \n",
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "{schema}\"\"\"\n",
    " \n",
    "def create_conversation(sample):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n",
    "      {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
    "    ]\n",
    "  }\n",
    " \n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "dataset = dataset.shuffle().select(range(12500))\n",
    " \n",
    "# Convert dataset to OAI messages\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n",
    "# split dataset into 10,000 training samples and 2,500 test samples\n",
    "dataset = dataset.train_test_split(test_size=2500/12500)\n",
    " \n",
    "print(dataset[\"train\"][345][\"messages\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import setup_chat_format\n",
    " \n",
    "# Hugging Face model id\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\" # or `mistralai/Mistral-7B-v0.1`\n",
    " \n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    " \n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    " \n",
    "# # set chat template to OAI chatML, remove if you start from a fine-tuned model\n",
    "model, tokenizer = setup_chat_format(model, tokenizer, format=\"chatml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
      "SCHEMA:\n",
      "CREATE TABLE table_1637981_7 (detroit__dtw_ VARCHAR, grand_rapids__grr_ VARCHAR)<|im_end|>\n",
      "<|im_start|>user\n",
      "When Grand Rapids's fare was $377.29, what is the fare to Detroit?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "SELECT detroit__dtw_ FROM table_1637981_7 WHERE grand_rapids__grr_ = \"$377.29\"<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(\n",
    "    dataset[\"train\"][345][\"messages\"],\n",
    "    tokenize=False,\n",
    "    # chat_template=\"chatml\"\n",
    "    add_generation_prompt=True\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'You are an text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_1637981_7 (detroit__dtw_ VARCHAR, grand_rapids__grr_ VARCHAR)',\n",
       "  'role': 'system'},\n",
       " {'content': \"When Grand Rapids's fare was $377.29, what is the fare to Detroit?\",\n",
       "  'role': 'user'},\n",
       " {'content': 'SELECT detroit__dtw_ FROM table_1637981_7 WHERE grand_rapids__grr_ = \"$377.29\"',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][345]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning-KL8mpKMW-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
